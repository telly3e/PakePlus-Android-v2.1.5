[
    {
        "id": "King's%20College%20London.html",
        "title": "King's College London",
        "content": "",
        "path": "King's College London"
    },
    {
        "id": "7CCEMEMB%20Embedded%20System%20Desig.html",
        "title": "7CCEMEMB Embedded System Design",
        "content": "",
        "path": "King's College London / 7CCEMEMB Embedded System Design"
    },
    {
        "id": "7CCEMEMB%20Embedded%20System%20Design/%E7%AC%94%E8%AE%B0.html",
        "title": "笔记",
        "content": "7CCEMEMB EMBEDDED SYSTEM DESIGN\n\n\nI. INTRODUCTION\n\nEmbedded systems are a combination of computer hardware and software designed for a specific function.\n\n\nTYPES OF DIFFERENT EMBEDDED SYSTEMS:\n\n * Microcontroller\n * Microprocessor\n * System on a chip (SoCs)\n * Field Programmable Gate Arrays (FPGA)\n\n\nCHALLENGES IN EMBEDDED SYSTEM DESIGN:\n\n * Sophisticated(精密的) functionality\n * Real-time operation\n * Low manufacturing cost\n * Low power consumption\n * Often have to run sophisticated algorithms on multiple platforms\n * Often provide sophisticated user interfaces\n\n\nREAL-TIME EMBEDDED SYSTEMS:\n\ncorrectness of the system behaviour not only depends\n\n * on the logical results of the computations\n * on the time at which the result are produced\n\nHARD REAL-TIME SYSTEMS\n\n * Must always meet deadlines (critical task)\n * System fails if deadline window is missed. Example: airplane sensor, autopilot systems and spacecrafts.\n\nSOFT REAL-TIME SYSTEM:\n\n * Must try to meet deadlines (desirable task)\n * The system does not fail if few deadlines are missed.\n * Example, audio and video streams\n\n\nPOWER MANAGEMENT\n\nModern microprocessors can control power consumption by\n\n * disabling unused circuits and\n * dynamically trimming clock speeds\n\nSoftware techniques:\n\n * Dynamic voltage and frequency scaling (DVFS) to match system power consumption with required performance.\n\n\nEMBEDDED SYSTEMS DESIGN METHODOLOGIES\n\n * Top-down design: (start from most abstract description; work to most detailed)\n * Bottom-up design: (work from small components to big system)\n\nREQUIREMENTS FORM / SHEET\n\nItemsDescriptionName Purpose Inputs Outputs Functions Performance\nManufacture Costs Physical Size/weight Power\n\nExamples:\n\nItemsDescriptionNameGPS moving mapPurposeConsumer-grade moving map for driving useInputsPower button, two control buttonsOutputsBack-lit LCD 400×600FunctionsUses 5-receiver GPS system; three user-selectable resolutions; always displays current latitude and longitudePerformanceUpdates screen within 0.25 seconds upon movementManufacture Costs£40Physical Size/weightNo more than 2×6 inches, 340gPower100mW\n\n\nREQUIREMENT\n\nInformal description of what customer needs using plain language. It could be developed via interviewing customers, talking to marketing representatives, or getting feedback from customers. Requirements phase links customers with designers\n\n\nSPECIFICATION\n\nPrecise description of what design team should deliver.\n\nThe requirements may be functional and non-functional.\n\n * Non-functional\n   * time required to compute output\n   * power consumption\n   * manufacturing cost\n   * physical size, weight\n   * time-to-market\n   * reliability\n * Functional\n   * input/output relationships\n\nMany specification styles:\n\n * Control-oriented vs. Data-oriented;\n * Textual vs. Graphical.\n\n\nHARVARD VS VON NEUMANN ARCHITECTURE\n\n * Harvard architecture has a separate program memory and data memory, which are accessed from separate buses.\n * This improves bandwidth over traditional von Neumann architecture, in which program and data are fetched from the same memory using the same bus.\n\n[api/attachments/BHmj5xnhG9wJ/image/image.png?2026-01-08%2015:14:36.636Z%22%3E]\n\n\nII. EMBEDDED SYSTEM ARCHITECTURE\n\n\nCOMPUTER\n\nA calculating machine that can\n\n * Accepts input information.\n * Processes the information according to a list of internally stored instructions\n * Produces the resulting output information.\n\n\nMICROPROCESSOR\n\nIt consists of only a central processing unit and uses an external bus to interface to RAM, ROM, and other peripherals. (e.g. CPU of a computer ...)\n\n * higher processing speed\n\n\nMICROCONTROLLER\n\n[api/attachments/ZwSlAgIoOm4k/image/image-1.png?2026-01-08%2015:14:36.640Z%22%3E]\n\n * more cost-effective\n * more efficient power saving (e.g. the control board of a washing machine / microwave oven ...)\n\n\nFUNCTIONAL UNITS OF A COMPUTER\n\n * Input\n   * Keyboard ...\n * Output\n   * monitor ...\n   * printer ...\n * ALU\n   * perform the desired operations on input info by instructions in the mem.\n * Control (coordinates various action)\n   * input\n   * output\n   * processing\n * Memory (stores information)\n   * instructions\n   * data\n\n\nHOW ARE THE FUNCTIONAL UNITS ARE CONNECTED?\n\nThey may be connected by a group of parallel wires which is called a bus. [api/attachments/t5n9xfgH1cvr/image/image_20251226_212032.png?2026-01-08%2015:14:36.638Z%22%3E]\n\nORGANISATION OF CACHE AND MAIN MEMORY\n\n[api/attachments/7O4Cl0NJ3NLU/image/image_20251226_211353.png?2026-01-08%2015:14:36.637Z%22%3E]\n\n\nARITHMETIC LOGIC UNITS\n\nALU has two units:\n\n * Arithmetic Units perform all arithmetic operations in a computer, and simple things like add one to a number.\n * Logic Units perform the logical operations and simple numerical tests such as negative or positive.\n\n\nHALF ADDER AND FULL ADDER\n\nComputers use binary for addition. Addition has two outputs: Sum (Sum) and Carry (Carry).\n\nHALF ADDER\n\n * can only handle the addition of two 1-bit binary numbers (A and B).\n * Cannot handle the carry-in bit.\n * Output: \\(Sum = A \\oplus B\\) (different or), \\(Carry = A \\cdot B\\) (and).\n\nFULL ADDER\n\n * Can handle three inputs: two additions A, B, and the previous carry-in bit (\\(C_{in}\\)).\n\nDIFFERENCES\n\nA full adder can be \"cascaded\" to form a multi-bit adder (e.g., 8-bit or 64-bit addition), whereas a half adder is usually used for lowest-bit calculations only.\n\n\nHOW CONTROL UNIT WORKS?\n\n * Fetch: Reads the address of the next instruction to be executed from memory.\n * Decode: Converts an instruction into an electronic control signal that determines what operation is to be performed (addition, jump, move data, etc.).\n * Fetch Operands: If the instruction requires data, the control unit directs the data to be fetched from memory or a register.\n * Execute: Sends a signal to the relevant component (e.g. ALU) to perform the task.\n * Store/Write-back: Stores the result back into a register or memory.\n\n\n\n\n\nIII. MEMORY ARCHITECTURE\n\n\nMEMORY FUNDAMENTALS & HARDWARE (LECTURE 3)\n\n1. MEMORY HIERARCHY (存储层次结构)\n\nMemory is organized in a pyramid based on speed, capacity, and cost.\n\n * CPU Registers (寄存器): The top of the pyramid. Fastest access, smallest capacity, located inside the CPU.\n * Cache (缓存): L1, L2, L3 SRAM. Very fast, bridges the speed gap between CPU and Main Memory.\n * Main Memory (主存): DRAM. Larger capacity, slower than cache.\n * Secondary Memory (二级存储): Flash or Magnetic (HDD). Slowest, largest capacity, non-volatile.\n\n> Note: In embedded systems, code execution usually happens directly from Flash Memory.\n\n2. BUILDING BLOCKS (构建模块)\n\n * Logic Gates (逻辑门): The basic unit (AND, OR, NOT) used to create memory circuits.\n * Latch (锁存器): Created by combining gates. It can store 1 bit of data. A \"Gated Latch\" has a \"Write Enable\" line.\n * Register (寄存器): A group of latches sharing a common \"Write Enable\" line. An 8-bit register holds 8 bits of data.\n * RAM Construction: A matrix of latches addressed by Row and Column selectors (Multiplexer).\n\n3. MEMORY CHARACTERISTICS (存储特性)\n\n * Capacity (容量): The amount of storage. Embedded systems typically range from Kilobytes to Megabytes.\n * Volatility (易失性):\n   * Volatile (易失性): Loses data when power is off (e.g., SRAM, DRAM, Registers).\n   * Non-Volatile (非易失性): Retains data without power (e.g., ROM, Flash, EEPROM).\n * Access Method (访问方式): Random Access (随机访问) allows accessing any location instantly given an address.\n * Latency (延迟): The time taken to transfer a single word of data to/from memory.\n\n\n\n\n\nMEMORY ARCHITECTURE & SEGMENTS (LECTURE 4)\n\n1. THE PLATFORM & MEMORY MAP (平台与内存映射)\n\n * Platform: Includes the CPU, the Integrated Circuit (IC), and Peripherals.\n * Memory Map (内存映射): The CPU views memory as a linear array of addresses. Different address ranges are assigned to Flash (Code), SRAM (Data), and Peripherals.\n\n2. CPU REGISTERS (ARM CORTEX-M0+)\n\nRegisters are the most tightly integrated memory.\n\n * General Purpose (通用寄存器): R0 - R12, used for operations and operands.\n * Special Purpose (专用寄存器):\n   * SP (R13 - Stack Pointer): Points to the top of the stack (MSP or PSP).\n   * LR (R14 - Link Register): Stores return addresses for function calls.\n   * PC (R15 - Program Counter): Tracks the address of the current instruction.\n\n3. MEMORY SEGMENTS (内存段)\n\nA program is divided into specific segments in memory:\n\n * Code Segment (代码段):\n   * Located in Flash (Non-volatile).\n   * Read-Only.\n   * Stores instructions and constants.\n * Data Segment (数据段):\n   * Located in SRAM (Volatile).\n   * Stores variables and the stack.\n   * Sub-segments of Data Memory:\n     1. Stack (栈): Temporary storage for local variables and function parameters. It grows down.\n     2. Heap (堆): Dynamic storage (e.g., malloc). It grows up.\n     3. Data: Initialized global and static variables (Non-zero).\n     4. BSS: Zero-initialized or uninitialized global/static variables.\n\n4. VARIABLE SCOPE & LIFETIME (变量作用域与生命周期)\n\n * Global Variables (全局变量): Allocated at compile time. Exist for the lifetime of the program. Stored in Data or BSS segments.\n * Local Variables (局部变量): Allocated at run-time. Exist only during the function/block execution. Stored in the Stack.\n\n\nSUMMARY Q&A\n\n1. How the memory works? Memory works by using digital circuits to store binary states (0s and 1s). It starts with basic logic gates (AND/OR/NOT) which form latches. Multiple latches form registers (to store bytes), and massive arrays of these latches form RAM, allowing data to be written or read based on specific addresses selected by a multiplexer .\n\n2. What are different types of memory? Memory is primarily categorized by volatility:\n\n * Volatile Memory: Requires power to maintain data (e.g., SRAM for cache/internal data, DRAM for main memory).\n * Non-Volatile Memory: Retains data without power (e.g., Flash for code storage, EEPROM, ROM).\n\n3. What is the Memory hierarchy in embedded systems? It is a structure balancing speed and cost:\n\n * Top: CPU Registers (Fastest, Smallest).\n * Middle: SRAM (Fast, used for Data/Stack/Heap).\n * Bottom: Flash (Slower, High Capacity, used for Code storage) .\n\n4. How we communicate (read/write) with memory? The CPU communicates via a bus controller using three main signals:\n\n * Address Bus: Specifies where to read/write.\n * Data Bus: Carries the actual information.\n * Control Bus (Read/Write/Enable): Signals whether to retrieve data or store it.\n\n5. What is Data Memory vs. Code Memory?\n\n * Code Memory (Flash): Stores the program instructions and constants. It is non-volatile and usually Read-Only during runtime.\n * Data Memory (SRAM): Stores variable data (variables, stack, heap). It is volatile and allows Read/Write operations during execution.\n\n6. Stack vs. Heap?\n\n * Stack: Used for static memory allocation (local variables, function calls). It is ordered (LIFO - Last In First Out) and managed automatically by the compiler. It grows downwards in memory.\n * Heap: Used for dynamic memory allocation (user-managed via malloc/free). It is unordered and grows upwards in memory.\n\n\n\n\n\nIIII. I/O ARCHITECTURE AND PROGRAM EXECUTION\n\n\n1. I/O ARCHITECTURE (输入/输出架构)\n\nDIGITAL TO ANALOGUE CONVERTERS (DAC, 数模转换器)\n\nDACs convert a digital binary signal (0s and 1s) into a continuous analog output signal.\n\n * Mechanism: Uses components like String Resistor Ladders (电阻梯) with switches to select a voltage level corresponding to the digital input.\n * Key Formula: \\(V_{out} = V_{ref} \\times (\\frac{D}{2^n})\\) where \\(D\\)\n   is the digital value and \\(n\\) is the number of bits.\n * Common Errors (常见误差):\n   * Offset Error (失调误差): Constant difference from the ideal output.\n   * Gain Error (增益误差): Difference proportional to the signal magnitude.\n   * Integral Non-Linearity (INL, 积分非线性): Deviation of the characteristic curve from a straight line.\n\nANALOGUE TO DIGITAL CONVERTERS (ADC, 模数转换器)\n\nADCs convert real-world analog signals (voltage) into digital data for the processor.\n\n * Process Flow:\n   1. Transducer (传感器): Converts physical quantity to voltage.\n   2. Signal Conditioning (信号调理): Amplifies and filters noise.\n   3. Sample and Hold (采样保持): Uses a capacitor to hold the voltage steady during conversion.\n * Types of ADC:\n   * Flash ADC (闪存型 ADC):\n     * Uses a bank of Comparators (比较器) and a Priority Encoder (优先编码器) in parallel.\n     * Pros: Extremely fast (up to 30 Msps).\n     * Cons: Expensive and complex (number of comparators doubles with each added bit).\n   * Successive Approximation Register ADC (SAR ADC, 逐次逼近型 ADC):\n     * Uses a comparator and a DAC to test bits one by one (binary search).\n     * Pros: Reasonable cost, accurate, low power. Used in Raspberry Pi Pico (12-bit).\n * Sampling Theory (采样理论):\n   * Quantization (量化): Approximating continuous values to discrete steps. Max error is\n     \\(\\frac{1}{2} V_{LSB}\\).\n   * Nyquist Theorem (奈奎斯特定理): Sampling frequency must be at least twice the highest signal frequency (\\(f_s \\ge 2f_{max}\\)) to avoid Aliasing (混叠).\n\nPULSE WIDTH MODULATION (PWM, 脉冲宽度调制)\n\nA digital technique to simulate analog voltage by controlling the \"ON\" time of a pulse.\n\n * Duty Cycle (占空比): The ratio of \"ON\" time to the total period.\n   * Longer ON time = Higher average voltage/speed.\n * Applications: Controlling motor speed, LED brightness.\n * RP2040: Has 8 PWM slices (16 outputs) and all GPIOs can be configured as PWM.\n\n\n2. PROGRAM EXECUTION (程序执行)\n\nTHE INSTRUCTION CYCLE (指令周期)\n\nThe processor executes a program by repeating a cycle:\n\n 1. Load/Fetch (取指): Read the instruction from memory.\n 2. Decode (译码): Figure out the operation and required data.\n 3. Execute (执行): Perform the computation (ALU operations like Add, Sub).\n 4. Next Instruction: Update the Program Counter (PC) to the next address or jump target.\n\n\nSUMMARY Q&A\n\n 1. What is the I/O interface in embedded system?\n\nThe I/O interface consists of mechanisms and buses (like AHB/APB bridges) that allow the CPU to communicate with external peripherals (sensors, motors) and memory, enabling the system to interact with the physical world.\n\n 1. How Digital to Analogue Converters (DAC) works?\n\nA DAC takes a binary number (digital code) and closes specific switches in a resistor network (like a string resistor ladder). This selects a specific voltage level from a reference voltage to output an analog signal proportional to the digital input.\n\n 1. What is the Analogue to Digital Converters (ADC)?\n\nAn ADC is a device that samples a continuous analog voltage at fixed intervals and quantizes it into a discrete digital binary value. It is essential for processing sensor data (temperature, sound, etc.).\n\n 1. How we use the Pulse Width Modulation (PWM)?\n\nPWM is used to control the power delivered to a load (like a motor or LED) without dissipating heat via resistance. By switching a digital output ON and OFF rapidly and varying the Duty Cycle, we can change the average voltage and current seen by the load, effectively controlling speed or brightness efficiently.\n\n 1. Program execution?\n\nProgram execution is the continuous process where the CPU fetches instructions from memory (Code/Flash), decodes them to understand the command, fetches necessary data from registers or memory, executes the operation (arithmetic or logic), and then moves to the next instruction.\n\n\n\n\n\nV. EMBEDDED NETWORKING AND COMMUNICATIONS\n\n\n1. FINITE STATE MACHINE (FSM / 有限状态机)\n\n * Definition: A model of behavior consisting of states, transitions, and actions to represent systems that change state in response to inputs.\n * Elements: Finite set of states (including an initial state), inputs, outputs, state transition logic, and output logic .\n * Moore Machine (摩尔型有限状态机): A specific type of FSM where current output values are determined only by the current state.\n\n\n2. I/O OPERATIONS (输入/输出操作)\n\n * Mechanism: Uses buffer registers (DATAIN/DATAOUT) and status control flags (SIN/SOUT).\n * Input: SIN flag sets to 1 when data is in the buffer; cleared when the processor reads it .\n * Output: SOUT flag indicates the display is ready to receive a character. [api/attachments/Cr8V1UvRUbKq/image/image_20251227_161849.png?2026-01-08%2015:17:06.691Z%22%3E]\n\n\n3. I/O SYNCHRONIZATION (I/O 同步)\n\nSynchronization bridges the speed mismatch between communicating devices. There are five main mechanisms:\n\n 1. Blind Cycle (盲周期): Software waits a fixed time assuming I/O completes. Used for predictable, short tasks (e.g., LCD, stepper motors) .\n 2. Busy Wait (忙等待): Software loop continuously checks the I/O status flag until done.\n 3. Interrupt (中断): Hardware triggers a special software execution (ISR) when data is ready or the device is idle.\n 4. Periodic Polling (周期性轮询): Uses a clock interrupt to check I/O status at regular intervals.\n 5. Direct Memory Access (DMA / 直接存储器访问): Transfers data directly between I/O and memory without CPU intervention.\n\n[api/attachments/mAV7ifpoihdS/image/image_20251227_170248.png?2026-01-08%2015:14:36.639Z%22%3E]\n\n\n4. I/O ACCESS MODES (I/O 访问模式)\n\n * Programmed I/O (程序控制 I/O): CPU controls the whole transfer and stays in a loop waiting for the device. Wasteful of CPU time .\n * Interrupt-Driven I/O (中断驱动 I/O): Peripheral initiates transfer via signal; CPU only stops current execution when needed. More efficient .\n * DMA: Specialized unit handles block transfers. CPU sets start address/size, and DMA interrupts CPU only upon completion .\n\n\n5. BUSES (总线)\n\n * Definition: Communication path interconnecting CPU, memory, and I/O.\n * Types: Control bus, Address bus, Data bus .\n * Timing Protocols:\n   * Synchronous (同步):\n     * The timing information for all devices are driven from a common clock signal.\n     * One data transfer can take place during one bus cycle.\n   * Asynchronous (异步): Uses handshaking signals (Master-ready and Slave-ready) instead of a common clock .\n\n\n6. PARALLEL PORT (并行端口)\n\n * Function: Transfers multiple bits (e.g., 8 or 16) simultaneously.\n * Pros/Cons: Faster per cycle but suffers from cross-talk (electrical interference), bulky cables, and signal degradation over distance.\n * Usage: Used for Keyboards (Input) and Printers (Output) using status lines like Valid or Idle.\n\n\n7. SERIAL PORT (串行端口)\n\n * Function: Transmits data one bit at a time. The speed is defined by the Bit Rate (比特率).\n * UART (Universal asynchronous receiver transmitter 通用异步收发传输器):\n   * Frame Structure: Idle -> Start Bit (0) -> Data Bits (usually 8) -> Parity Bit (Optional) -> Stop Bit (1).\n   * FIFO (First In First Out / 先入先出): Buffers data to separate production and consumption rates, improving efficiency.\n   * Parity: Used for error checking (Even/Odd). Detects single-bit errors but cannot locate them.\n\n[api/attachments/O0GTqeZm0pLF/image/image_20251227_171633.png?2026-01-08%2015:14:36.639Z%22%3E]\n\nExample: A UART port is configured with the parameters:\n\n * baud rate: 9600\n * data bits: 8\n * stop bit: 1\n * start bit: 1\n * parity bit: none How long does it take to transmit 5k bytes data?\n\n\\[5000 \\times \\frac{8+1+1}{9600}\\]\n\n\n8. SERIAL STANDARDS\n\n * RS-232: Single-ended signaling. Low speed (max 20 Kbps at 50ft), short distance, point-to-point (DTE to DCE).\n * RS-485: Differential signaling (noise cancellation). High speed (\\(10~Mbit/s\\)), long distance (4000 ft), used in industrial applications.\n\n\n9. BOARD-LEVEL PROTOCOLS\n\n * SPI (Serial Peripheral Interface / 串行外设接口):\n   * Wires: 4 - Serial Clock (SCK), Slave Select (SS), Master Out Slave In (MOSI), Master In Slave Out (MISO).\n   * Features: Master/Slave, high speed, simple shift register logic.\n   * Cons: No addressing, no flow control, no acknowledgements, no error checking.\n\n[api/attachments/10oxaXD2Eq1B/image/image_20251227_172233.png?2026-01-08%2015:14:36.639Z%22%3E]\n\n * I2C (Inter-Integrated Circuit / 集成电路总线):\n   * Wires: 2 - SCL (clock line) and SDA (data line).\n   * Features: Address-based (supports multiple devices), uses Start/Stop conditions and ACKs.\n   * Use case: Lower speed, multiple simple sensors.\n   * SCL functions as a clock line and SDA can function as a 1-bit serial data line or as a 1-bit serial address line. A common ground is also required. [api/attachments/LWxqmJEQpRue/image/image_20251227_172442.png?2026-01-08%2015:14:36.639Z%22%3E]\n   * How I2C works ?\n     * Master issues a START condition (First pulls SDA low, then pulls SCL low). Master writes an address to the bus. Add a bit indicating whether it wants to read or write. Slaves that do not match address do not respond. A matching slave issues an ACK by pulling down SDA.\n     * Either master or slave transmits one byte Receiver issues an ACK. This step may repeat.\n     * Master issues a STOP condition First releases SCL, then releases SDA At this point the bus is free for the other transaction\n\n\n10. HIGH-LEVEL PROTOCOLS\n\n * Medium-end: Modbus, Profibus, CAN.\n * High-end: Ethernet, Profinet.\n\n\nSUMMARY Q&A\n\n1. What is the finite state machine? A Finite State Machine (FSM) is a software model used to represent systems that transition between a finite number of states based on input events. It helps separate the system's \"policies\" (what to do) from \"mechanisms\" (how to do it).\n\n2. What are input/output operations in embedded systems? These are operations where the processor transfers data to and from external devices (like keyboards or displays) using buffer registers (DATAIN/DATAOUT) and synchronizes the transfer using status flags (SIN/SOUT) to indicate when data is valid or the device is ready.\n\n3. What are the synchronization algorithms for software and hardware? The main algorithms are:\n\n * Blind Cycle: Waiting a fixed time.\n * Busy Wait: Loops checking a status flag.\n * Interrupt: Hardware signals the CPU to stop and service the device.\n * Periodic Polling: Checking status at fixed clock intervals.\n * DMA: Hardware handles transfer directly to memory .\n\n4. What are ways to access I/O devices and how each one is working?\n\n * Programmed I/O: The software controls the entire transfer, keeping the CPU busy in a loop.\n * Interrupt-Driven: The peripheral signals the CPU when it is ready, allowing the CPU to do other work until interrupted.\n * DMA: A dedicated controller transfers blocks of data between I/O and memory without the CPU's active involvement.\n\n5. What are the buses and how they will work? Buses are communication paths (Data, Address, Control) connecting the CPU, memory, and I/O. They work via Synchronous timing (using a shared clock) or Asynchronous timing (using \"Master-ready\" and \"Slave-ready\" handshaking signals).\n\n6. What is parallel and serial port and what is the difference between them? A Parallel port transfers multiple bits (e.g., 8) simultaneously, making it faster per cycle but bulky and prone to interference. A Serial port transfers data one bit at a time sequentially. Serial ports are generally smaller, cheaper, and better for longer distances.\n\n7. What is embedded network? An embedded network refers to the communication protocols and physical interfaces used to connect microcontrollers to sensors, actuators, and other controllers. It ranges from low-level chip protocols (UART, SPI, I2C) to industrial standards (RS-485, CAN, Modbus).\n\n8. What is the difference between I2C and SPI and where to use them?\n\n * SPI uses 4 wires, is faster, and has no addressing overhead; it is best for high-bandwidth streams (like A/D converters).\n * I2C uses 2 wires and device addresses; it is slower but uses fewer pins and is better for connecting many simple sensors on a board.\n\n9. What are mid and high-end communication protocols?\n\n * Mid-end: Modbus, Profibus, and CAN (often used in industrial automation).\n * High-end: Ethernet and Profinet (used for high-speed, complex networking) .\n\n\n\n\n\nVI. REAL TIME OPERATING SYSTEMS AND SCHEDULING\n\n\n1. DATA CHECKING METHODS (数据校验方法)\n\nReliability is crucial in embedded systems. Several methods are used to ensure data integrity during transmission.\n\n * Parity Check (奇偶校验):\n   * Mechanism: An extra bit (parity bit) is added to the data unit.\n     * Even Parity (偶校验): The total number of 1s (including the parity bit) must be even.\n     * Odd Parity (奇校验): The total number of 1s must be odd.\n   * Limitation: It is a simple method but weak. It can detect single-bit errors but fails if an even number of bits are flipped (e.g., two errors might cancel each other out).\n * Checksum (校验和):\n   * Mechanism: The sender adds up the data segments and takes the 1's complement of the sum to generate the checksum. The receiver performs the same addition including the checksum; the result should be all 0s (in 1's complement arithmetic).\n   * Advantage: Better than parity checks as it can detect more complex error patterns.\n\n[api/attachments/QuyAQr5VFrR9/image/image_20251228_191925.png?2026-01-08%2015:14:36.639Z%22%3E]\n\n[api/attachments/sCl9wT0MpWkQ/image/image_20251228_192008.png?2026-01-08%2015:14:36.640Z%22%3E]\n\n * Cyclic Redundancy Check - CRC (循环冗余校验):\n   * Mechanism: Uses polynomial division (binary division with XOR operations). The sender divides the data by a \"divisor\" (polynomial) and appends the remainder to the data. The receiver divides the received message by the same divisor; if the remainder is 0, the data is correct.\n   * Advantage: Highly effective at detecting burst errors and validates the order of bits (bit sequence), which simple parity cannot do.\n\n\n2. REAL-TIME OPERATING SYSTEMS - RTOS (实时操作系统)\n\n * Definition: A system where correctness depends not only on the logical result (逻辑结果) but also on the time (时间) at which the result is produced.\n * Classifications:\n   1. Soft Real-Time (软实时): Missing a deadline results in lower quality of service but is not catastrophic 灾难性的 (e.g., email delivery).\n   2. Hard Real-Time (硬实时): Missing a deadline causes total system failure or catastrophic consequences (e.g., airbag deployment, chemical plant pressure valve).\n   3. Firm Real-Time (固实时): Lies between soft and hard. A few missed deadlines are tolerable (quality loss), but too many lead to failure (e.g., video streaming dropping frames).\n\n\n3. THREADS (线程)\n\n * Concept: A thread is a \"program in action\" (dynamic). While a Program (程序) is static code in ROM, a Thread breathes life into it using the processor.\n * Structure: Each thread has its own Stack (栈) to store:\n   * Registers (e.g., PC, SP, PSR).\n   * Local variables.\n * Shared Resources: Threads share global memory and I/O but have private stacks.\n\n\n4. TASK STATES (任务状态)\n\nA thread transitions between different states controlled by the OS:\n\n * Active/Ready (就绪): Ready to run but waiting for the CPU.\n * Run (运行): Currently executing.\n * Blocked (阻塞): Waiting for an external event (e.g., I/O, keyboard).\n * Sleep (休眠): Waiting for a fixed amount of time.\n\n\n5. REAL-TIME METRICS\n\n * Latency (延迟): Time difference between when an event occurs and when the task starts running (\\(\\Delta_i = T_i - E_i\\)).\n * Jitter (抖动): The variation in timing; difference between the desired run time and the actual run time.\n\n\n6. SCHEDULING ALGORITHMS (调度算法)\n\nThe Scheduler (调度器) creates the illusion of concurrent processing (并行处理).\n\n * Rate Monotonic Scheduling - RMS (速率单调调度):\n   * Type: Static Priority (静态优先级).\n   * Rule: Priority is determined by the Period (周期). Shorter period = Higher priority.\n   \n   * Schedulability Test: Sufficient but not necessary.\n     \n     \\[\\sum_{i=1}^n \\frac{C_i}{P_i} \\le n(2^{\\frac{1}{n}}-1)\\]\n     \n     (Utilization must be below a bound, approx 0.69 for large \\(n\\)).\n * Earliest Deadline First - EDF (最早截止时间优先):\n   * Type: Dynamic Priority (动态优先级).\n   * Rule: Priority is based on the absolute Deadline (截止时间). The task with the closest deadline runs first.\n   \n   * Schedulability Test: Necessary and sufficient.\n     \n     \\[\\sum_{i=1}^n \\frac{C_i}{P_i} \\le 1\\]\n     \n     (Total utilization must be less than or equal to 100%).\n\n\n7. PRIORITY INVERSION (优先级反转)\n\n * Problem: A high-priority task is blocked by a low-priority task that holds a shared resource (critical section). If a medium-priority task preempts the low-priority task, the high-priority task is delayed indefinitely.\n * Solution: Priority Inheritance Protocol (优先级继承协议). When a low-priority task blocks a high-priority task, it temporarily inherits the high priority until it releases the resource.\n\n\n8. MULTI-CORE SYSTEMS (多核系统)\n\n * Purpose: To handle increasing computational demands and allow parallel processing.\n * Structure: Multiple cores share caches and interconnects to process data (e.g., CATS/DOGS example in slides).\n\n\n9. NETWORK FLOW SCHEDULING (网络流调度)\n\n * Concept: Scheduling is modeled as a graph problem (\\(G(V,E)\\)) with a Source (\\(s\\)) and a Sink (\\(t\\)).\n * Mapping:\n   * Nodes: Jobs (\\(J\\)) and Time Frames (\\(F\\)).\n   * Edges: Represent capacity (execution time).\n * Goal: Maximize flow from Source to Sink. If Max Flow equals the total execution time of all tasks, the set is schedulable.\n * Algorithm: Ford-Fulkerson Algorithm.\n   * Finds Augmenting Paths (增广路径) in the Residual Graph (残余图).\n   * Max Flow Min Cut Theorem: The maximum flow is equal to the capacity of the minimum cut (bottleneck).\n\n\n10. ENERGY EFFICIENT SCHEDULING (节能调度)\n\n * Motivation: Processors use CMOS technology where dynamic power is the bottleneck.\n   * Power \\(\\propto V^2 \\times F\\) (Voltage squared times Frequency).\n * DVS / DVFS (动态电压频率调整): Dynamic Voltage (and Frequency) Scaling.\n   * Technique to reduce power by lowering voltage/frequency when peak performance is not needed.\n   * Trade-off: Lower frequency means tasks take longer to execute (\\(t_{execution}\\)\n     increases), but power drops quadratically, resulting in net energy savings.\n * Static Voltage Scaling: Scaling factors are calculated offline to ensure utilization\n   \\(\\le 1\\) at the reduced frequency.\n\n\nSUMMARY Q&A\n\n1. How Check Sum and CRC works for data check?\n\n * Checksum: It works by summing up the data values being transmitted. The result (often the 1's complement of the sum) is sent along with the data. The receiver performs the same calculation; if the calculated sum matches the received checksum (result is zero), the data is assumed correct.\n * CRC (Cyclic Redundancy Check): It uses polynomial division. The data bits are treated as a large number and divided by a predetermined binary number (the polynomial). The remainder of this division is the CRC code appended to the message. The receiver divides the incoming message by the same number; a zero remainder indicates no errors. CRC is particularly good at detecting burst errors and bit-order errors.\n\n2. What is parity check and why it is not a good approach?\n\n * Parity Check is adding a single bit to a data block to make the total number of 1s either even (even parity) or odd (odd parity).\n * Why it is not good: It is not robust because it can only detect an odd number of bit errors (e.g., 1, 3, 5 errors). If two bits flip (e.g., a 0 becomes 1 and a 1 becomes 0), the parity count remains the same, and the error goes undetected.\n\n3. What is a real-time operation system?\n\n * An RTOS is an operating system where the correctness of the system depends not only on the logical results of the computations but also on the time at which those results are produced. It is designed to process data as it comes in, typically without buffering delays, to meet strict deadline constraints (Hard, Firm, or Soft).\n\n4. What are the Threads and how they are used in RTOS?\n\n * Threads are \"light-weight processes\" or programs in execution. Unlike static programs, threads have dynamic states (registers, stack, local variables). In an RTOS, multiple threads are used to perform different functions (tasks) concurrently. The RTOS switches the processor between these threads to achieve the desired system functionality.\n\n5. What is Scheduler?\n\n * A scheduler is a kernel function in the Operating System responsible for deciding which thread should run at any given time. It switches between threads (context switching) to give the illusion of simultaneous execution (concurrency) on a single processor.\n\n6. Explain RMS and EDF scheduling.\n\n * RMS (Rate Monotonic Scheduling): A static priority algorithm for periodic tasks. It assigns priorities based on the task's period: the shorter the period, the higher the priority. It is optimal for static priorities.\n * EDF (Earliest Deadline First): A dynamic priority algorithm. It assigns priorities based on the absolute deadline of the current job: the task with the closest deadline gets the highest priority. It can achieve 100% CPU utilization.\n\n7. What is priority inversion problem?\n\n * Priority Inversion occurs when a high-priority task is forced to wait for a lower-priority task to complete. This usually happens when the low-priority task holds a lock on a shared resource that the high-priority task needs. If a medium-priority task preempts the low-priority task while it holds the lock, the high-priority task is blocked for an indefinite period, potentially causing deadline misses.\n\n8. What are multi-core processors and how they work?\n\n * Multi-core processors are single chips containing two or more independent processing units (cores). They work by allowing multiple instructions or threads to run in parallel on separate cores, sharing resources like memory and interconnects, which increases overall performance and efficiency compared to single-core systems.\n\n9. How Network flow formulation work?\n\n * Network flow formulation models the scheduling problem as a directed graph with a source and a sink. Jobs and Time Frames are represented as nodes. Edges represent the capacity (execution time) available. The problem is solved by finding the \"Maximum Flow\" from source to sink. If the max flow equals the total execution time required by all jobs, a valid schedule exists. Algorithms like Ford-Fulkerson are used to calculate this flow.\n\n10. What is Energy Efficient Scheduling?\n\n * Energy Efficient Scheduling involves managing the processor's execution speed to minimize energy consumption while still meeting task deadlines. The primary technique discussed is Dynamic Voltage Scaling (DVS) (or DVFS), where the processor's voltage and frequency are lowered when the workload is light. Since power consumption is proportional to the square of the voltage (\\(V^2\\)), reducing the speed slightly results in significant energy savings.",
        "path": "King's College London / 7CCEMEMB Embedded System Design / 笔记"
    },
    {
        "id": "7CCEMTN1%20Telecommunications%20Ne.html",
        "title": "7CCEMTN1 Telecommunications Networks I",
        "content": "",
        "path": "King's College London / 7CCEMTN1 Telecommunications Networks I"
    },
    {
        "id": "7CCEMTN1%20Telecommunications%20Networks%20I/%E7%AC%94%E8%AE%B0.html",
        "title": "笔记",
        "content": "QUEUEING THEORY\n\nQueueing theory allows us to perform theoretical analysis on the network and understand the performance of the system.\n\ndistribution of waiting time distribution of number of customers in the system idle time busy time system utilization\n\nNetwork Delay = Queueing Delay + Propagation Delay (depends on the distance) + Processing Delay during service time\n\n\nCHARACTERISTICS OF QUEUEING PROCESS\n\n * Arrival Pattern and Service Pattern of Customers\n   * The probability distribution of arrivals and service time\n * Queueing Disciplines\n   * First come, first served (FCFS)\n   * Priority queue\n   * Preemptive/non-preemptive\n * System Capacity\n   * Finite / infinite waiting room (i.e., number of packets in the queue)\n\n符号 (Symbol)定义 (Definition)备注 / 公式 (Notes / Formulas)\\(\\lambda\\)mean arrival rate of customers/time unit\n\\(\\mu\\)mean service rate in customers/time unit\n\\(n(t)\\)number of customers in the system at time \\(t\\)\n\\(\\rho\\)server utilization\\(\\rho < 1\\)\nfor stability\\(L\\)Average number of customers in systems\n\\(L_q\\)Average number of customers in the queuesnote that\n\\(L = L_q + \\rho\\)\\(W\\)Average delay in system (includes server + queue)\n\\(W_q\\)Average delay in queuenote that \\(W = W_q + 1/\\mu\\)\n\n\nSTOCHASTIC PROCESS\n\n1. 定义 DEFINITION\n\n随机过程 (Stochastic Process) 是指一组由时间 \\(t\\) 索引的随机变量集合 \\(X(t)\\)。\n\n * 时间域 \\(t \\in T\\):\n   * 连续 (Continuous): 时间是连续的实数。\n   * 离散 (Discrete): 时间是一系列离散的点 (例如: \\(t = 1, 2, 3...\\) 或 7:00, 8:00...)。\n * 直观理解: 可以将其视为一个“被时间索引的随机变量家族”。\n * 分布函数:\n   * \\(F_X(x;t) = P[X(t) \\le x]\\)\n   * (注: 幻灯片中标注为PDF，但公式 \\(P[X \\le x]\\) 数学上通常定义为累积分布函数 CDF)\n\n2. 随机过程的分类 CLASSIFICATION\n\n根据变量随时间变化的特性，主要分为以下五类：\n\n2.1 平稳过程 (STATIONARY PROCESSES)\n\n * 定义: 统计特性独立于时间。系统的性质不会随时间推移而改变。\n\n * 公式:\n   \n   \\[F_X(x; t + \\tau) = F_X(x; t)\\]\n\n2.2 独立过程 (INDEPENDENT PROCESSES)\n\n * 定义: 不同时间点的变量是相互独立的。\n\n * 公式: 联合分布等于各自分布的乘积。\n   \n   \\[F_X(x; t) = F_{X1}(x_1; t_1) \\dots F_{Xn}(x_n; t_n)\\]\n\n2.3 马尔可夫过程 (MARKOV PROCESSES)\n\n * 定义: 具有无记忆性 (Memoryless)。未来的状态仅取决于当前状态，而与过去的历史状态无关。\n * 公式:\n\n\\[P[X(t_{n+1}) = x_{n+1} \\mid X(t_n) = x_n, \\dots, X(t_1) = x_1] = P[X(t_{n+1}) = x_{n+1} \\mid X(t_n) = x_n]\\]\n\n2.4 生灭过程 (BIRTH-DEATH PROCESSES)\n\n * 定义: 状态转移只发生在相邻的状态之间。\n * 应用: 常用于排队论（人数增加1或减少1）。\n\n2.5 随机游走 (RANDOM WALKS)\n\n * 定义: 过程的下一个位置等于前一个位置加上一个随机变量（步长）。\n * 特点: 步长是从任意分布中独立抽取的。\n\n3. LITTLE'S LAW\n\n\\[\\text{Mean Number of Customers} = \\text{Arrival Rate} \\times \\text{Mean Response Time}\\]\\[N = \\lambda \\times T\\]\n\n4. 基础分布与过程\n\n4.1 泊松分布 (POISSON DISTRIBUTION)\n\n * 类型: 离散概率分布。\n * 用途: 描述固定时间间隔内事件发生的次数。\n * 公式: \\(P(X=k) = \\frac{\\lambda^k e^{-\\lambda}}{k!}\\)\n   * \\(k\\): 事件发生次数\n   * \\(\\lambda\\): 平均发生率 (Intensity)\n\n4.2 指数分布 (EXPONENTIAL DISTRIBUTION)\n\n * 类型: 连续概率分布。\n * 用途: 描述泊松过程中连续两个事件发生的时间间隔 (Interarrival time)。\n * 概率密度 (PDF): \\(f(x) = \\lambda e^{-\\lambda x}\\)\n * 累积分布 (CDF): \\(F(x) = 1 - e^{-\\lambda x}\\)\n * 均值: \\(E[\\tau] = 1/\\lambda\\)\n\n4.3 泊松过程 (POISSON PROCESS)\n\n * 定义: 计数过程 \\(X(t)\\)，表示在 \\((0, t]\\) 时间段内发生的事件数。\n * 参数: \\(\\lambda t\\)\n * 核心定理: 如果到达数服从泊松分布，则到达间隔时间服从指数分布。\n\n4.4无记忆性 (MEMORYLESS PROPERTY)\n\n * 公式: \\(P(X > a+b | X > a) = P(X > b)\\)\n * 含义: 未来的等待时间与已经等待的时间无关。指数分布是唯一具有此性质的连续分布。\n\n5. 平衡方程 (BALANCE EQUATIONS)\n\n用于分析系统处于稳态 (Equilibrium) 时的概率分布。\n\n5.1 全局平衡 (GLOBAL BALANCE)\n\n * 核心思想: 对于任意状态 \\(j\\)，离开该状态的速率 = 进入该状态的速率。\n\n * 公式:\n   \n   \\[P_j \\sum_{i \\neq j} P_{ji} = \\sum_{i \\neq j} P_i P_{ij}\\]\n   \n   (Rate Out = Rate In)\n\n5.2 一般平衡 (GENERAL BALANCE)\n\n * 对于状态空间中的任意子集 \\(S\\)，进入 \\(S\\) 的总流速等于离开 \\(S\\) 的总流速。\n\n6. 生灭过程 (BIRTH-DEATH PROCESS)\n\n一种特殊的状态连续变化过程，状态只能在相邻整数间转移 (\\(k \\leftrightarrow k+1\\))。\n\n * \\(\\lambda_k\\) (Birth Rate): 从状态 \\(k \\to k+1\\) 的速率（到达）。\n * \\(\\mu_k\\) (Death Rate): 从状态 \\(k \\to k-1\\) 的速率（服务完成）。\n\n6.1 稳态解推导\n\n利用局部平衡方程（在状态 \\(k\\) 和 \\(k+1\\) 之间切一刀）：\n\n 1. 流速平衡: \\(P_k \\lambda_k = P_{k+1} \\mu_{k+1}\\)\n 2. 递归关系: \\(P_{k+1} = \\frac{\\lambda_k}{\\mu_{k+1}} P_k\\)\n\n 3. 通解 (用 \\(P_0\\) 表示):\n    \n    \\[P_k = P_0 \\prod_{i=0}^{k-1} \\frac{\\lambda_i}{\\mu_{i+1}}\\]\n\n 4. 求解 \\(P_0\\): 利用归一化条件 \\(\\sum_{k=0}^{\\infty} P_k = 1\\)。\n    \n    \\[P_0 = \\frac{1}{1 + \\sum_{k=1}^{\\infty} \\prod_{i=0}^{k-1} \\frac{\\lambda_i}{\\mu_{i+1}}}\\]\n\n6.2 系统性能指标\n\n * 平均系统人数 (\\(\\bar{N}\\)): \\(\\sum k P_k\\)\n * Little's Law (利特尔定律): \\(T = \\frac{\\bar{N}}{\\lambda}\\) (平均逗留时间 = 平均人数 / 到达率)\n * 平均等待时间 (\\(w\\)): \\(w = T - \\frac{1}{\\mu}\\) (总时间 - 服务时间)\n\n7. 马尔可夫链与排队系统 (MARKOV CHAINS & QUEUING SYSTEMS)\n\n这是将概率理论应用于实际系统建模的关键步骤。\n\n7.1 为什么使用生灭过程建模？\n\n排队系统的运作机制与生灭过程（Birth-Death Process）完全对应，通过以下映射关系建立模型：\n\n现实中的排队系统 (Queuing System)数学模型 (Birth-Death Process)对应的概率分布顾客到达 (Arrivals)“生”过程 (Birth Process)通常建模为 泊松过程 (Poisson)\n(参数:\n\\(\\lambda\\))服务完成 (Services)“灭”过程 (Death Process)服务时间通常建模为 指数分布 (Exponential)\n(参数: \\(\\mu\\))\n\n7.2 建模结论\n\n * 系统性质: 一个拥有泊松到达流和指数服务时间的排队系统，本质上就是一个生灭过程。\n * 马尔可夫性 (Markov Property): 由于泊松和指数分布的无记忆性，系统未来的状态仅取决于当前状态，而与过去无关。这使得我们可以用马尔可夫链的状态转移图来分析排队问题。\n\n7.3 状态转移图解\n\n * 状态 (\\(k\\)): 系统中的总人数（排队者 + 正在被服务者）。\n * \\(\\lambda\\) (向右箭头): 导致状态 \\(k \\to k+1\\) 的速率（有人来了）。\n * \\(\\mu\\) (向左箭头): 导致状态 \\(k \\to k-1\\) 的速率（有人走了）。\n\n[api/attachments/1PhJjWKBEiZk/image/image_20260102_151312.png?2026-01-08%2014:50:19.513Z%22%3E]\n\n> 总结: 只要知道到达率 \\(\\lambda\\) 和服务率 \\(\\mu\\)，我们就可以画出状态转移图，并利用之前推导的平衡方程求出系统处于各种状态的概率。\n\n\nM/M/1 MODEL\n\n1. 模型定义 (MODEL DEFINITION)\n\nM/M/1 是最基础的单服务台排队系统。\n\n * M (Arrival): 泊松到达 (Poisson Arrivals)，参数 \\(\\lambda\\)。\n * M (Service): 指数服务时间 (Exponential Service)，参数 \\(\\mu\\)。\n * 1: 单个服务台 (Single Server)。\n * 稳定性条件: \\(\\lambda < \\mu\\) (也就是 \\(\\rho < 1\\))，否则队列无限发散。\n\n2. 关键参数\n\n * 流量强度 / 利用率 (\\(\\rho\\)):\n   \n   \\[\\rho = \\frac{\\lambda}{\\mu}\\]\n   * \\(\\rho\\): 系统忙碌的概率 (Server Busy)。\n   * \\(1-\\rho\\): 系统空闲的概率 (\\(p_0\\), Server Idle)。\n\n3. 稳态概率分布 (PROBABILITY DISTRIBUTION)\n\n基于生灭过程的平衡方程 (\\(\\lambda p_n = \\mu p_{n+1}\\))，系统中有 \\(n\\) 个顾客的概率为：\n\n * 通项公式:\n   \n   \\[p_n = (1 - \\rho)\\rho^n\\]\n * 物理意义: 顾客数量服从几何分布 (Geometric Distribution)。\n\n4. 关键性能指标 (PERFORMANCE METRICS)\n\n指标符号公式说明平均系统人数\\(\\bar{N}\\) 或 \\(L\\)\\[\\frac{\\rho}{1-\\rho}\\]包含正在排队和正在接受服务的人。当\n\\(\\rho \\to 1\\) 时呈指数爆炸。平均逗留时间\\(T\\) 或 \\(W\\)\\[\\frac{1}{\\mu - \\lambda}\\]\n\n也是\n\n\\[\\frac{\\bar{N}}{\\lambda}\\]\n\n(Little's Law)。包含排队时间和服务时间。\n\n平均排队时间\\(W_q\\)\\[T - \\frac{1}{\\mu} = \\frac{\\rho}{\\mu - \\lambda}\\]仅计算在队列中等待的时间。系统人数\n\\(\\ge k\\) 的概率\\(P(n \\ge k)\\)\\[\\rho^k\\]用于计算缓冲区溢出概率。\n\n5. 核心结论与直觉\n\n * 非线性增长: 系统延迟并不是随着负载线性增加的。当负载 (\\(\\rho\\)) 超过 0.8 或 0.9 时，延迟 (\\(T\\)) 和 队列长度 (\\(\\bar{N}\\)) 会急剧恶化 。\n * 设计启示: 永远不要设计一个利用率接近 100% 的系统，必须留有余量 (Headroom)，否则微小的波动都会导致巨大的延迟。\n\n\nM/M/1/K MODEL\n\nM/M/1/K 模型描述了一个单服务台、系统容量有限的排队系统。当系统内顾客数达到 \\(K\\) 时，新到达的顾客将被拒绝（丢失）。\n\n1. 系统参数与假设\n\n * 到达率 (\\(\\lambda\\)): 顾客到达服从泊松过程。\n * 服务率 (\\(\\mu\\)): 服务时间服从指数分布。\n * 系统容量 (\\(K\\)): 系统允许的最大顾客数（包含正在服务的顾客）。\n   * 队列容量: \\(K-1\\)（因为有1个在服务中）。\n * 服务规则: 先到先得 (FCFS)。\n\n2. 有效到达率 (EFFECTIVE ARRIVAL RATE)\n\n由于系统容量限制，到达率是状态依赖的（State-dependent）：\n\n\\[\\lambda_k = \\begin{cases} \\lambda, & k < K \\quad (\\text{系统未满，允许进入}) \\\\ 0, & k \\ge K \\quad (\\text{系统已满，拒绝进入}) \\end{cases}\\]\n\n3. 状态转移图 (STATE TRANSITION DIAGRAM)\n\n这是一个有限状态的生灭过程：\n\n * 状态空间：\\(\\{0, 1, 2, \\dots, K\\}\\)\n * 生（Birth）速率：\\(\\lambda\\) (在状态 \\(0\\) 到 \\(K-1\\))\n * 灭（Death）速率：\\(\\mu\\) (在状态 \\(1\\) 到 \\(K\\))\n\n[api/attachments/YOuPitESixoE/image/image_20260102_195722.png?2026-01-08%2014:50:19.513Z%22%3E]\n\n4. 稳态概率公式 (STEADY-STATE PROBABILITIES)\n\n令流量强度 (Traffic Intensity) \\(\\rho = \\frac{\\lambda}{\\mu}\\)\n\n4.1 状态 \\(K\\) 的概率 (\\(P_K\\))\n\n根据生灭过程平衡方程：\n\n\\[P_k = P_0 \\rho^k, \\quad 0 \\le k \\le K\\]\\[P_k = 0, \\quad k > K\\]\n\n4.2空闲概率 (\\(P_0\\))\n\n利用归一化条件 \\(\\sum_{k=0}^{K} P_k = 1\\)，即 \\(P_0 \\sum_{k=0}^{K} \\rho^k = 1\\)。\n\n这是一个有限项等比数列求和，结果为：\n\n * 当 \\(\\rho \\neq 1\\) 时:\n   \n   \\[P_0 = \\left[ \\frac{1 - \\rho^{K+1}}{1 - \\rho} \\right]^{-1} = \\frac{1 - \\rho}{1 - \\rho^{K+1}}\\]\n\n * 当 \\(\\rho = 1\\) 时:\n   \n   \\[P_0 = \\frac{1}{K+1}\\]\n\n5. 关键性能指标 (PERFORMANCE METRICS)\n\n * 阻塞概率/丢失概率 (\\(P_K\\) - Blocking Probability): 系统处于满员状态的概率，也就是顾客被拒绝的概率。\n   \n   \\[P_K = \\frac{(1 - \\rho) \\rho^K}{1 - \\rho^{K+1}}\\]\n\n * 系统平均顾客数 (\\(L\\) - Average Number in System):\n   \n   \\[L = \\sum_{k=0}^{K} k P_k\\]\n   \n   (注：该公式比 M/M/1 复杂，因为是截断的级数)\n\n * 有效吞吐量 (\\(\\lambda_{eff}\\)): 实际进入系统的平均到达率。\n   \n   \\[\\lambda_{eff} = \\lambda (1 - P_K)\\]\n\n\nM/M/\\(\\INFTY\\) 与 M/M/M MODEL\n\n本节涵盖了两种从 M/M/1 扩展出的重要排队模型：无限服务台模型 (Responsive) 与 多服务台模型 (m-Server Case)。\n\n1. M/M/\\(\\INFTY\\) 模型 (无限服务台)\n\n[api/attachments/M6TbuZaThiEy/image/image_20260102_220115.png?2026-01-08%2014:50:19.515Z%22%3E]\n\n1.1 定义\n\n系统拥有无限的服务资源，或者对于每个到达的顾客，总有一个新的服务台可用。\n\n * 特点： 无需排队，即时服务。\n * 应用： 自助服务系统、云计算资源分配。\n\n1.2 系统参数\n\n * 到达率：\\(\\lambda_k = \\lambda\\)\n * 服务率：\\(\\mu_k = k\\mu\\) (系统总服务率与在场顾客数成正比)\n\n1.3 稳态分布 (泊松分布)\n\n系统处于状态 \\(k\\)（有 \\(k\\) 个顾客）的概率服从泊松分布：\n\n\\[P_k = \\frac{\\rho^k}{k!} e^{-\\rho}\\]\n\n其中 \\(\\rho = \\lambda / \\mu\\)。\n\n1.4 空闲概率 (\\(P_0\\))\n\n\\[P_{0} = e^{-\\rho}\\]\n\n1.5 性能指标\n\n * 系统平均顾客数 (\\(N\\)): \\(N = \\lambda / \\mu = \\rho\\)\n * 平均逗留时间 (\\(T\\)): \\(T = 1 / \\mu\\) (即等于平均服务时间，无等待时间)\n\n2. M/M/M 模型 (多服务台)\n\n[api/attachments/h2DXcMTsOQdS/image/image_20260102_220052.png?2026-01-08%2014:50:19.514Z%22%3E]\n\n2.1 定义\n\n系统拥有 \\(m\\) 个并行的服务台，共用一个等待队列 (Single Queue, Multiple Servers)。\n\n状态依赖的服务率 (\\(\\mu_k\\)):\n\n系统的处理能力根据在场人数 \\(k\\) 与服务台数 \\(m\\) 的关系而变化：\n\n\\[\\mu_k = \\begin{cases} k\\mu, & k \\le m \\quad (\\text{服务台未满，类似 } M/M/\\infty) \\\\ m\\mu, & k > m \\quad (\\text{服务台全忙，达到最大产能}) \\end{cases}\\]\n\n2.2 稳态概率公式\n\n由于服务率分段，概率公式也分为两部分：\n\n 1. 当 \\(k \\le m\\):\n    \n    \\[P_k = P_0 \\frac{1}{k!} \\left(\\frac{\\lambda}{\\mu}\\right)^k\\]\n\n 2. 当 \\(k > m\\):\n    \n    \\[P_k = P_0 \\frac{1}{m! m^{k-m}} \\left(\\frac{\\lambda}{\\mu}\\right)^k\\]\n\n2.3 空闲概率 (\\(P_0\\))\n\n计算较为复杂，需满足 \\(\\rho = \\frac{\\lambda}{m\\mu} < 1\\) 才能稳定：\n\n\\[P_0 = \\left[ \\sum_{k=0}^{m-1} \\frac{(m\\rho)^k}{k!} + \\frac{(m\\rho)^m}{m!(1-\\rho)} \\right]^{-1}\\]\n\n2.4 关键指标：排队概率 (PROBABILITY OF QUEUEING)\n\n表示所有 \\(m\\) 个服务台都被占用，新顾客需要等待的概率 (Erlang-C)：\n\n\\[\\text{Pr}[\\text{queueing}] = \\sum_{k=m}^{\\infty} P_k = \\frac{P_0 (m\\rho)^m}{m!(1-\\rho)}\\]\n\n2.5 MARKOV CHAIN (推导)\n\n\\[\\lambda_k = \\lambda\\]\\[\\mu_k = \\left\\{ \\begin{array}{ll} k\\mu & \\text{if } k \\le m \\\\ m\\mu & \\text{if } k > m \\end{array} \\right.\\]\\[\\text{For } k \\le m, \\quad p_k = p_0 \\frac{\\lambda}{\\mu} \\frac{\\lambda}{2\\mu} \\ldots \\frac{\\lambda}{k\\mu} = p_0 \\left(\\frac{\\lambda}{\\mu}\\right)^k \\frac{1}{k!}\\]\\[\\text{For } k > m, \\quad p_k = p_0 \\frac{\\lambda}{\\mu} \\frac{\\lambda}{2\\mu} \\ldots \\frac{\\lambda}{m\\mu} \\ldots \\frac{\\lambda}{m\\mu} = p_0 \\left(\\frac{\\lambda}{\\mu}\\right)^k \\frac{1}{m!} \\left(\\frac{1}{m}\\right)^{k-m}\\]\n\n\nM/M/M/M MODEL (损失制系统)\n\n[api/attachments/wVQubtGzK1p8/image/image_20260102_222409.png?2026-01-08%2014:50:19.515Z%22%3E]\n\n1. 模型定义\n\nM/M/m/m 模型（有时记为 M/M/m/loss）描述了一个拥有 \\(m\\) 个服务台但没有排队缓冲区的系统。\n\n * 别名: m-Server Loss System (m服务台损失系统)。\n * 典型应用: 电话网络 telephony system（线路全忙即占线）、电路交换。\n\n2. 系统机制\n\n * 无等待: 顾客到达时，如果有空闲服务台，立即服务；如果所有 \\(m\\) 个服务台全忙，顾客直接丢失 (Lost)，不能排队。\n * 参数设定:\n   * 到达率: \\(\\lambda_k = \\lambda\\) (当 \\(k < m\\)), \\(\\lambda_k = 0\\) (当\n     \\(k \\ge m\\))。\n   * 服务率: \\(\\mu_k = k\\mu\\) (所有在场顾客同时接受服务)。\n\n3. 稳态概率分布\n\n基于生灭过程推导，系统处于状态 \\(k\\)（有 \\(k\\) 个顾客在服务中）的概率为：\n\n\\[P_k = P_0 \\frac{(\\lambda/\\mu)^k}{k!}, \\quad 0 \\le k \\le m\\]\\[P_k = 0, \\quad k > m\\]\n\n其中 \\(P_0\\) (系统全空的概率) 为归一化常数：\n\n\\[P_0 = \\left[ \\sum_{k=0}^{m} \\frac{(\\lambda/\\mu)^k}{k!} \\right]^{-1}\\]\n\n4. 爱尔朗损失公式 (ERLANG'S LOSS FORMULA)\n\n这是该模型最关键的性能指标，用于计算所有服务台都忙 (System Busy) 的概率，即顾客被阻塞/丢失的概率。\n\n通常被称为 Erlang's B Formula，记为 \\(B(m, \\lambda/\\mu)\\)：\n\n\\[P_m = \\frac{\\frac{(\\lambda/\\mu)^m}{m!}}{\\sum_{k=0}^{m} \\frac{(\\lambda/\\mu)^k}{k!}}\\]\n * 分子: 流量强度 \\(\\rho^m\\) 除以 \\(m!\\)。\n * 分母: 截断的指数级数求和（从 0 到 \\(m\\)）。\n * 意义: \\(P_m\\) 既是系统处于满员状态的时间比例，也是被丢失的顾客占总到达顾客的比例。\n\n\nNETWORK OF QUEUES\n\n1. 受阻到达模型 (DISCOURAGED ARRIVALS)\n\n[api/attachments/ePQKyJM27UuT/image/image_20260103_153122.png?2026-01-08%2014:50:19.515Z%22%3E]\n * 定义：随着系统中顾客/请求数量的增加，新到达者产生“受阻”心理，到达率降低。\n * 模型参数：\n   * 到达率：\\(\\lambda_k = \\frac{\\alpha}{k+1}\\)（随人数 \\(k\\) 增加而减少）。\n   * 服务率：\\(\\mu_k = \\mu\\)（恒定）\n * 关键指标：\n   * 稳态概率：\n     * \\[P_k=\\frac{\\left(\\alpha/\\mu\\right)^k}{k!}\\cdot e^{-\\frac{\\alpha}{\\mu}}\\]\n   * 平均系统人数：\\(\\overline{N} = \\frac{\\alpha}{\\mu}\\)\n   * 有效到达率：\\(\\overline{\\lambda} = \\mu[1 - e^{-(\\alpha/\\mu)}]\\)\n   * 平均停留时间：利用 Little's Law\n     * \\[T=\\frac{\\overline{N}}{\\lambda}=\\frac{\\alpha/\\mu}{\\mu(1-e^{-\\alpha/\\mu})}\\]\n\n\n\n\n\n2. 排队网络与其挑战\n\n * 基本概念：网络中有多个传输队列相互作用，离开一个队列的流量进入下一个队列。\n * 相关性难点 (Correlation)：\n   * 在进入第一个队列后，数据包的到达间隔与其长度变得强相关。\n   * 慢车效应 (The Slow Truck Effect)：长数据包（慢车）需要长服务时间，导致短数据包（快车）在后面排队。这种现象破坏了后续队列作为简单 M/M/1 模型分析的前提（即破坏了无记忆性）。\n\n3. 网络分析的三大定理\n\n3.1 KLEINROCK 独立性近似\n\n * 假设：在密集连接的网络中，多个数据流的合并（merging）能够恢复到达时间和服务时间的独立性。\n * 适用条件：泊松到达、指数分布包长、网络连接紧密、中等到重度负载。\n * 结论：可以将每条链路视为独立的 M/M/1 队列进行近似计算。\n\n3.2 BURKE 定理\n\n * 核心内容：一个到达率为 \\(\\lambda\\) 的 M/M/1 系统的离去过程 (Departure Process) 也是一个速率为\n   \\(\\lambda\\) 的泊松过程。\n * 推论：任意时刻 \\(t\\)，系统中的顾客数与 \\(t\\) 时刻之前的离去序列是相互独立的。这允许我们将串联的队列解耦分析。\n\n3.3 JACKSON 定理 (JACKSON'S THEOREM)\n\n * 场景：开放式网络，包含外部泊松到达 \\(r_i\\) 和节点间的概率路由 \\(P_{ij}\\)\n\n * 乘积形式解 (Product Form Solution)：\n   \n   网络处于状态 \\(n=(n_1, ..., n_k)\\) 的稳态概率是各个节点独立概率的乘积：\n   \n   \\[P(n_1, ..., n_k) = P_1(n_1) \\cdot P_2(n_2) \\cdot ... \\cdot P_k(n_k)\\]\n   \n   \n   \n   \\[P(n) = \\prod_{i=1}^{K} \\rho_i^{n_i}(1-\\rho_i)\\]\n   \n   这表明网络中的每个节点确实可以像独立的 M/M/1 队列一样处理\n\n4. 开放网络流量分析 (OPEN NETWORK ANALYSIS)\n\n * 流量守恒方程：\n   \n   对于节点 \\(i\\)，总到达率 \\(\\lambda_i\\) 等于外部输入加上来自网络其他节点的输入：\n   \n   \\[\\lambda_i = \\gamma_i + \\sum_{j=1}^{m} \\lambda_j r_{ji}\\]\n   \n   其中 \\(\\gamma_i\\) 是外部输入，\\(r_{ji}\\) 是从 \\(j\\) 到 \\(i\\) 的路由概率\n\n * 矩阵求解：\n   \n   令 \\(\\lambda\\) 为总流量向量，\\(\\gamma\\) 为外部流量向量，\\(R\\) 为路由矩阵：\n   \n   \\[\\lambda = \\gamma + \\lambda R \\implies \\lambda(I-R) = \\gamma\\]\\[\\lambda = \\gamma(I-R)^{-1}\\]\n   \n   通过此公式可唯一确定每个队列的总负荷\n\n5. 性能指标计算\n\n一旦求出每个节点的 \\(\\lambda_i\\) 和利用率 \\(\\rho_i = \\lambda_i / \\mu_i\\)，即可计算全网指标：\n\n * 全网平均顾客数：\\(\\tilde{N} = \\sum_{i=1}^{M} \\frac{\\rho_i}{1-\\rho_i}\\)\n * 全网平均延迟：\\(\\tilde{D} = \\frac{\\tilde{N}}{L}\\)，其中 \\(L\\)\n   是网络总外部负荷 (\\(\\sum \\gamma_i\\))\n * 包含传播延迟：如果节点间有固定的传播延迟 \\(d_{ij}\\)，公式需修正为包含传输时间\n\n6. 典型计算示例\n\n * 反馈回路 (Feedback Loop)：当存在从队列 2 返回队列 1 的概率 \\(q\\) 时，必须建立联立方程组求解 \\(\\lambda_1\\) 和\n   \\(\\lambda_2\\)\n * 概率分流与合流：\n   * 泊松流的概率分流 (Splitting) 产生独立的泊松流\n   * 独立泊松流的合流 (Merging) 产生新的泊松流，其速率为各流之和\n\n\nINTERNET ARCHITECTURE & EVOLUTION\n\n\n1. 互联网的演变 (EVOLUTION OF THE INTERNET)\n\n互联网的发展经历了从基础网络连接到AI融合的多个里程碑：\n\n * 起源 (1960s-1980s):\n   * 1969年：DARPA委托建立了第一个分组交换网络 ARPANET。\n   * 1978年：TCP分裂为TCP和IP两个协议，提高了效率和可扩展性。\n   * 1986年：NSF建立NSFNET骨干网，连接主要大学。\n * 商业化与普及 (1990s):\n   * 1991年：Tim Berners-Lee发明万维网 (WWW)。\n   * 1995年：Amazon和eBay上线，电子商务开启。\n * 移动与现代网络 (2000s-Present):\n   * 2007年：iPhone发布加速了移动互联网的普及。\n   * 2010年：4G/LTE使得视频流媒体成为主流。\n   * 2020年：5G网络广泛铺开。\n   * 未来展望：6G (2025+) 将融合AI和物联网，支持沉浸式AR/VR。\n\n\n2. 互联网拓扑结构 (INTERNET TOPOLOGY)\n\n互联网是一个松散的分层结构 (Hierarchical Structure) ：\n\n * Level 1: 网络接入点 (NAPs) - 全球和国家流量的核心互连枢纽 。\n * Level 2: 国家骨干网 (National Backbones) - 连接NAP到区域提供商的高容量网络。\n * Level 3: 区域 ISP - 区域内的商业连接提供商。\n * Level 4: 本地 ISP - 连接终端用户和企业。\n * Level 5: 终端用户网络 (LANs) - 家庭或办公网络。\n\n\n3. 网络架构原则 (NETWORK ARCHITECTURE)\n\n3.1 定义与目标\n\n * 网络架构是一套指导协议和算法工程设计的高层原则。\n * 早期设计目标主要为了解决互连问题，而非一开始就设计了完美的全局架构。\n\n3.2 协议栈 (PROTOCOL STACK)\n\n 1. 应用层 (Application): 提供网络服务 (如HTTP, FTP)。\n 2. 传输层 (Transport): 管理端到端通信和数据流 (如TCP, UDP)。\n 3. 网络层 (Internet): 处理寻址和路由 (如IP)。\n 4. 链路层 (Link Layer): 管理物理传输 (如Wi-Fi, 5G)。\n\n3.3 核心原则\n\n * 端到端原则 (End-to-End Principle): 保持核心网络简单 (\"dumb network\")，将智能放在边缘 (设备、应用)。\n   * 优势: 简化核心设施降低成本，允许边缘创新 (如Web, AI工具) 而无需修改核心网络。\n\n\n4. 网络层：IP协议 (INTERNET PROTOCOL)\n\n * 特性: IP提供无连接 (Connectionless)、不可靠 (Unreliable) 的数据包投递服务。\n * IP头部关键字段:\n   * TTL (Time to Live): 防止数据包在网络中无限循环。\n   * Protocol: 标识上层协议 (如TCP/UDP)。\n   * Source/Destination IP: 发送方和接收方地址。\n * 分片与重组 (Fragmentation & Reassembly):\n   * 链路层有最大传输单元 (MTU) 限制 (如以太网MTU为1500字节)。\n   * 当IP包超过链路MTU时需要分片，重组任务由接收端完成，以保持路由器效率。\n   * 缺点: 任何一个分片丢失都需要重传整个包，且增加处理负载。\n\n\n5. 传输层：TCP协议 (TRANSMISSION CONTROL PROTOCOL)\n\nTCP提供可靠的 (Reliable)、面向连接的流传输服务。\n\n5.1 基本机制\n\n * 确认机制 (ACK): 发送包后启动计时器，等待接收端确认，否则重传。\n * 头部字段: 包含序列号 (Sequence number)、确认号 (Acknowledgment number) 和 窗口大小 (Window Size)。\n\n5.2 确认策略 (ACKNOWLEDGMENTS)\n\n * 累积确认 (Cumulative ACK): 收到按序到达的数据包后才生成新的确认。\n * 延迟确认 (Delayed ACK): 延迟ACK发送 (典型200ms) 以减少流量，或直到有数据回传。\n * 重复确认 (Duplicate ACK): 当收到乱序 (Out-of-Order) 数据包时，接收端会立即发送重复ACK (不延迟)。\n\n5.3 流量控制 (FLOW CONTROL)\n\n * 基于滑动窗口 (Sliding Window) 机制。\n * 窗口大小取决于接收端的缓存能力 (Advertised Window)。\n\n\n6. TCP拥塞控制 (CONGESTION CONTROL)\n\n拥塞控制由发送端根据网络状况调整，目的是防止网络过载。\n\n6.1 核心概念\n\n * 带宽时延积 (BDP): \\(BDP = Bandwidth \\times RTT\\)。它衡量了网络中“在途”数据的最佳容量。\n * 最佳拥塞窗口 (Optimal CWND): 应近似于 BDP。\n\n6.2 丢包检测\n\nTCP通过两种方式检测丢包：\n\n 1. 重传超时 (RTO): 计时器超时未收到ACK。\n    * \\(RTO = Mean\\ RTT + 4 \\times Mean\\ Deviation\\)\n    * 超时后采用指数退避 (Exponential Backoff)，即每次超时RTO翻倍。\n 2. 重复确认 (Duplicate acks): 连续收到3个重复ACK。\n\n控制阶段\n\n 1. 慢启动 (Slow Start):\n    * 初始 CWND = 1 MSS。\n    * 每收到一个ACK，窗口加1 (指数增长)。\n    * 持续直到达到慢启动阈值 (ssthresh)。\n 2. 拥塞避免 (Congestion Avoidance):\n    * 达到阈值后，窗口线性增长 (每个RTT增加1 MSS)。\n\n6.3 对丢包的反应\n\n * 超时 (Timeout) 发生时:\n   * CWND 重置为 1 MSS。\n   * ssthresh 设为当前窗口的一半。\n   * 重新进入慢启动。\n * 快速重传 (Fast Retransmit) 发生时 (收到3个Dupacks):\n   * 不等待超时，立即重传丢失包。\n   * CWND 减半 (而不是重置为1)。\n   * 进入拥塞避免阶段 (跳过慢启动，这称为快速恢复)。\n\n\nQUALITY OF SERVICE & IPV6\n\n\n1. 从 IPV4 到 IPV6 的演进 (FROM IPV4 TO IPV6)\n\n1.1 IPV6 的必要性\n\n * 地址枯竭：到2019年底，几乎所有可用的IPv4地址都已分配完毕，限制了互联网的增长。\n * 地址空间扩展：IPv6将IP地址长度从IPv4的32位扩展到了128位。这意味着地球表面每平方米可以分配约 \\(6.5 \\times 10^{23}\\)\n   个唯一地址。\n\n1.2 IPV6 的优势\n\n * 短期优势：\n   * 增加了地址空间，路由拓扑更高效。\n   * 架构针对64位进行了优化，报头固定大小，处理更高效。\n   * 改进的主机和路由器发现机制，以及即插即用（自动配置）功能 。\n * 长期优势：\n   * 移动性支持：为移动设备提供更高效的连接。\n   * QoS 支持：确保端到端的网络性能。\n   * 扩展性：支持VoIP、视频流等新应用需求。\n\n1.3 IPV6 报头格式\n\n * 尽管IPv6地址长度是IPv4的4倍，但其报头长度仅为IPv4的两倍。\n * 主要字段：包括版本、流量类别（Traffic Class）、流标签（Flow Label）、有效载荷长度、下一报头（Next Header）和跳数限制（Hop Limit）。\n\n\n2. 服务质量 (QUALITY OF SERVICE - QOS)\n\n2.1 为什么需要 QOS？\n\n * 现状：当前互联网主要采用“尽力而为”（Best Effort）模型，不保证时效性或交付，无服务区分，且存在拥塞和响应时间不可预测的问题。\n * 未来需求：新型应用（如AR、VR、远程医疗、自动驾驶）需要比简单的文件传输更严格的网络支持。\n * 定义 (ITU)：QoS是电信服务的综合特性，决定了其满足用户明确及隐含需求的能力。\n\n2.2 关键 QOS 参数\n\nQoS 包含技术指标和用户体验两个方面。\n\n * 吞吐量 (Throughput)\n * 传输延迟 (Transit delay)\n * 延迟抖动 (Jitter/Delay variation)\n * 错误率 (Error rate)：包括误码率 (BER) 和丢包率 (PER) 。\n\n\n3. QOS 架构 (QOS ARCHITECTURES)\n\n为了在未来互联网中实现QoS，主要有两种架构哲学：\n\n3.1 综合服务 (INTSERV - INTEGRATED SERVICES)\n\n这是一个有状态 (Stateful) 的架构。\n\n * 服务类别：\n   1. Guaranteed (保证服务)：指定最大延迟和带宽。\n   2. Controlled Load (受控负载)：无严格保证，但提供类似于无拥塞网络的合理服务水平。\n   3. Best Effort (尽力而为)：针对无特殊要求的应用。\n * RSVP (资源预留协议)：\n   * 用于建立连接时的资源请求（如缓冲区、带宽）。\n   * 由接收方负责请求QoS。\n   * 软状态 (Soft-state)：状态会超时（通常30秒），需定期刷新。\n   * 使用 Tspec (描述流量特征) 和 Rspec (描述预留需求) 进行动态协商。\n * 局限性：每个路由器都需要维护每个流的状态，导致扩展性差（Scalability issues）。\n\n3.2 区分服务 (DIFFSERV - DIFFERENTIATED SERVICES)\n\n这是一个无状态 (Stateless) 的架构，旨在解决IntServ的扩展性问题。\n\n * 核心理念：将服务聚合为类（Class），在网络边缘对数据包进行标记（利用IPv4/IPv6报头中的ToS/DS字段），网络核心仅根据标记进行处理。\n * DS 字段与 PHB：\n   * 使用DS字节（DS-byte）来决定每一跳的行为（Per Hop Behavior, PHB）。\n   * EF (Expedited Forwarding)：加速转发，构建“虚拟专线”，低延迟、低丢包。\n   * AF (Assured Forwarding)：确保转发，提供不同级别的交付保证和丢弃优先级。\n * 路由器组件：\n   * 边缘路由器：负责分类（Classification）、计量（Meter）、标记（Marker）和整形（Shaping）。\n   * 核心路由器：仅根据PHB进行调度（Scheduling）和队列管理（Queue Management）。\n\n\n4. 队列管理机制 (QUEUE MANAGEMENT)\n\n为了实现QoS，路由器需要智能地管理队列以避免拥塞。\n\n4.1 随机早期检测 (RED - RANDOM EARLY DETECTION)\n\n * 原理：监控平均队列长度，在缓冲区填满之前，按概率随机丢弃或标记数据包。\n * 目的：避免所有TCP流同时降低发送速率的“全局同步”现象，并保持低延迟和高吞吐量。\n * 阈值机制：\n   * 队列 < \\(min_{th}\\)：接受所有包。\n   * \\(min_{th}\\) ≤ 队列 ≤ \\(max_{th}\\)：概率性丢包。\n   * 队列 > \\(max_{th}\\)：丢弃所有包。\n\n4.2 RIO (RED WITH IN AND OUT)\n\n * RED的扩展，支持两个优先级（In-profile 和 Out-of-profile），用于服务区分。\n\n\n5. INTSERV 与 DIFFSERV 对比 (INTSERV VS DIFFSERV)\n\n特性IntServ (综合服务)DiffServ (区分服务)服务粒度单个流 (Individual flow)流聚合 (Aggregate of flows)路由器状态需维护每流状态 (调度、缓冲)无状态 (基于DS字段)保证类型确定性或统计性保证相对保证 (Better than best-effort)准入控制需要 (RSVP)不需要扩展性受限于流的数量 (Limited by flows)受限于服务类的数量 (Limited by classes)协调范围端到端逐跳/域内 (Local/Per-hop)\n\n\n6. 5G 核心网与 QOS (5G CORE NETWORK & QOS)\n\n * 5G 业务类型：\n   * eMBB (增强型移动宽带)：高吞吐量。\n   * URLLC (超高可靠低延迟通信)：极低延迟。\n   * mMTC (海量机器类通信)：高连接密度。\n * DiffServ 在 5G 中的角色：\n   * 5G核心网基于IP构建，利用DiffServ作为底层传输机制。\n   * 映射机制：将5G的 QoS Flow (QFI) 映射到IP层的 DSCP 标记。\n   * 例如：URLLC流量映射为EF（加速转发），eMBB映射为AF（确保转发）。\n\n\nROUTING ON THE INTERNET\n\n\n1. 核心概念\n\n * 路由器任务 (Tasks of a Router)\n   * 转发 (Packet Forwarding): 实时操作，查找转发表，将数据包从输入接口移至输出接口。\n   * 路由 (Routing): 全局决策，确定端到端路径，生成路由表，仅在拓扑或成本改变时触发。\n * 最小成本 (Least Cost): 将网络视为图，路径成本为链路成本之和（成本可基于跳数、容量或队列长度）。\n\n\n2. 路由策略分类 (ROUTING STRATEGIES)\n\n * 固定 (Fixed): 路径预设且固定，直到拓扑改变。\n * 洪泛 (Flooding): 向所有邻居转发。\n   * 优点: 鲁棒，必能找到最短跳数路径。\n   * 缺点: 产生大量流量。\n * 随机 (Random): 随机选择下一跳，无需网络信息。\n * 自适应 (Adaptive): 根据网络状况（拥塞、故障）动态调整，是现代网络的主流。\n\n\n3. 两大核心路由算法\n\nA. 链路状态算法 (LINK STATE) - DIJKSTRA\n\n * 原理: 每个路由器构建完整的网络拓扑图。\n * 信息交换: 路由器向网络中所有其他节点广播链路成本。\n * 算法流程:\n   1. 初始化：\\(T=\\{s\\}\\) (源节点)，计算邻居初始成本。\n   2. 选择：在集合 \\(T\\) 之外找到离源点 \\(s\\) 最近的节点，加入 \\(T\\)。\n   3. 更新：检查是否可以通过新加入的节点优化到其他节点的路径。\n      * 公式: \\(L(n) = \\min[L(n), L(x) + w(x,n)]\\)。\n\nB. 距离向量算法 (DISTANCE VECTOR) - BELLMAN-FORD\n\n * 原理: 仅基于邻居提供的信息进行迭代更新，无需全网拓扑。\n * 信息交换: 仅与直接邻居交换距离向量。\n * 算法流程:\n   * 基于跳数约束 \\(h\\) 逐步优化路径。\n   * 公式: \\(L_{h+1}(n) = \\min_{j} [L_{h}(j) + w(j,n)]\\)。\n   * 解释: 到节点 \\(n\\) 的新成本 = Min (到邻居 \\(j\\) 的成本 + 邻居 \\(j\\) 到 \\(n\\) 的链路成本)。\n\n\n4. 算法对比 (COMPARISON)\n\n特性Bellman-Ford (距离向量)Dijkstra (链路状态)所需信息邻居节点的链路成本及路径总成本整个网络的完整拓扑和所有链路成本信息交换对象仅与直接邻居交换与网络中所有节点交换更新机制基于邻居信息迭代更新本地计算最短路径树\n\n\n5. 实际协议: OSPF\n\n * 全称: Open Shortest Path First (开放式最短路径优先)。\n * 机制: 属于链路状态协议，使用 Dijkstra 算法计算最短路径。\n * 地位: 互联网中广泛使用的标准路由协议。\n\n\nMINIMUM SPANNING TREE\n\n\n1. 核心定义\n\n * 生成树 (Spanning Tree): 包含图中所有节点且无环的连通子图。\n * 最小生成树 (MST): 边的权重之和最小的生成树。\n * 输入: 连通的无向加权图 \\(G=(V, E)\\)。\n * 目标: 找到边集 \\(T \\subseteq E\\)，连接所有节点且 \\(\\sum_{e \\in T} w(e)\\) 最小。\n\n\n2. 经典算法 (GREEDY APPROACHES)\n\nA. PRIM 算法 (生长法)\n\n * 逻辑: 维护一个单一的树，从任意根节点开始，通过添加连接树与非树节点的最小权重边来扩展。\n * 关键机制:\n   * 使用最小优先队列 (Min-Priority Queue) 存储非树节点。\n   * 属性 u.key: 节点 \\(u\\) 到树中任意节点的最小边权重。\n   * 更新: 当节点加入树后，更新其邻居的 key 值。\n\nB. KRUSKAL 算法 (合并法)\n\n * 逻辑: 维护一个森林 (Forest)，按权重从小到大处理所有边。\n * 关键机制:\n   * 排序: 首先对所有边按权重排序。\n   * 查并集 (Union-Find): 用于判断两个节点是否属于同一棵树（防止环）。\n   * 操作: 若边 \\((u, v)\\) 连接两个不同的连通分量，则加入边并合并分量；否则丢弃。\n\n\n3. 算法对比 (PRIM VS KRUSKAL)\n\n特性Prim 算法Kruskal 算法操作对象节点 (Vertices)边 (Edges)过程形态一棵不断生长的树不断合并的森林 (多棵树)策略选择离当前树最近的节点选择全图权重最小且不构成环的边结果总权重与 Kruskal 相同总权重与 Prim 相同\n\n\n4. 应用与区别\n\n * MST vs 最短路径 (Shortest Path):\n   * MST: 最小化全网连接的总成本 (Global Cost)。\n   * 最短路径 (Dijkstra): 最小化源点到特定点的路径成本 (Point-to-Point Cost)。\n   * 注意: MST 中的两点间路径不一定是最短路径。\n * 典型应用:\n   * 基础设施建设: 以最低预算连通所有城市/站点（如电网、铺路）。\n   * 聚类分析 (Clustering): 通过断开 MST 中权值最大的边来划分数据簇。\n\n\nMULTI-HOP WIRELESS NETWORKS\n\n\n1. INTRODUCTION\n\n * Context: Routing in Mobile Ad hoc Networks (MANETs) is challenging due to node mobility and constant topology changes.\n * Categories:\n   * Proactive (Table-Driven): Maintains tables of all links. Updates immediately upon movement. High overhead but low latency for initial sending .\n   * Reactive (On-Demand): Finds routes only when needed. Uses caching. No static overhead but suffers from \"slow start\" delay .\n\n\n2. DYNAMIC SOURCE ROUTING (DSR)\n\nDSR is a Reactive protocol based on Source Routing.\n\n * Concept: The source node determines the complete path. The route is stored in the packet header. Intermediate nodes do not need routing tables .\n\nPROTOCOL OPERATIONS\n\n * Route Discovery:\n   * Source: Floods a Route Request (RREQ) containing sender ID, target ID, and Request ID.\n   * Intermediate Nodes: If no route to target, append own address to RREQ and rebroadcast. Discard duplicates to prevent loops.\n   * Destination: Upon receiving RREQ, sends a Route Reply (RREP) unicast back to the source containing the full accumulated path.\n * Route Maintenance:\n   * Link Failure: Detected when data forwarding fails (e.g., node moves out of range).\n   * Route Error (RERR): Generated by the node upstream of the break and sent to the source.\n   * Propagation: RERRs can be piggybacked on new RREQs to update other nodes' caches.\n\n\n3. OPTIMIZATIONS & CHALLENGES\n\n * Route Cache:\n   * Nodes learn routes by forwarding packets or overhearing (\"snooping\") traffic.\n   * Allows intermediate nodes to reply to RREQs locally, reducing network flooding.\n * Route Reply Storms:\n   * Problem: If multiple neighbors have a cached route, they all reply simultaneously, causing collision.\n   * Solution: Nodes utilize random delays before sending RREP.\n * Scalability: Packet header size grows with route length, which can be inefficient for very large networks.\n\n\nNETWORK VIRTUALIZATION & SOFTWARE DEFINED NETWORKING\n\n\n1. 核心问题：网络僵化 (NETWORK OSSIFICATION)\n\n * 现状： TCP/IP 架构在边缘（Web, App）取得了巨大成功，但核心网络层却停滞不前。\n * 创新的阻碍：\n   * 软硬捆绑： 设备商（如思科、华为）销售封闭的专用设备，外部无法编程或修改 。\n   * 极度复杂： 路由器代码量巨大，不仅昂贵且容易出错 。\n   * 周期漫长： 协议标准化过程极其缓慢（约10年），阻碍了新技术的快速部署 。\n   * 无法实验： 缺乏在大规模现网中测试新架构（如全新路由算法）的手段 。\n\n\n2. 解决方案：网络虚拟化 (NV)\n\n * 定义： 一种允许多个异构的虚拟网络在共享的物理基础设施上共存且相互隔离的环境 。\n * 角色分离的商业模式：\n   * 基础设施提供商 (InP)： 管理底层物理硬件。\n   * 服务提供商 (SP)： 租赁物理资源构建虚拟网络，提供端到端服务 。\n * 设计特性：\n   * 异构性： 可以在同一个物理网络上跑完全不同的协议（如IPv4和IPv6同时运行）。\n   * 递归性 (Recursion)： 虚拟网络中可以嵌套子虚拟网络。\n   * 重访 (Revisitation)： 物理节点可被多次映射到同一虚拟网络中 。\n\n\n3. 使能技术：软件定义网络 (SDN)\n\nSDN 是实现网络虚拟化的关键架构，其核心是控制与转发分离。\n\nA. 三层架构\n\n 1. 基础设施层 (数据平面)：\n    * 由简单的包转发硬件（交换机）组成。\n    * 只负责根据流表（Flow Table）转发数据，不进行复杂计算 。\n 2. 控制层 (控制平面)：\n    * 核心是 SDN控制器 (Network OS)。\n    * 集中管理网络状态，计算路由路径，并通过安全通道下发指令 。\n 3. 应用层：\n    * 各种网络应用（如流量工程、安全防火墙、移动性管理）。\n    * 通过控制器提供的API编程来定义网络行为 。\n\nB. 关键机制\n\n * 解耦 (Decomposition)： 将传统路由器中的“大脑”（控制）提取出来，只保留“四肢”（转发）。\n * 开放接口 (OpenFlow)： 这是一个标准协议，允许控制器直接编程底层的交换机硬件（例如：“若包头匹配X，则转发至端口Y”）。\n * 全局视图： 控制器拥有全网的实时视图，使得网络管理更加灵活和智能 。\n\n\n4. 总结：未来的网络行业\n\n * 类比计算机产业： 网络正在经历从“大型机”向“PC/操作系统”模式的转变。\n * 优势：\n   * 降低门槛： 不需要购买昂贵的专用设备，通用硬件即可。\n   * 加速创新： 可以在软件层面快速尝试新的网络协议和算法。\n   * 灵活管理： 像安装APP一样部署网络服务。\n\n",
        "path": "King's College London / 7CCEMTN1 Telecommunications Networks I / 笔记"
    },
    {
        "id": "7CCEMTN1%20Telecommunications%20Networks%20I/2025%20Mock%20Paper.html",
        "title": "2025 Mock Paper",
        "content": "EXAM PAPER & SOLUTIONS\n\n\nQUESTION ONE\n\n\nPART (A)\n\nContext: Consider a scenario where a client is communicating with a server using TCP (Transport Control Protocol). The client sends a SYN (Synchronise) packet to the server with sequence number 1000, the server responds with a SYN-ACK (Synchronise-Acknowledgment) packet, with an acknowledgment number of 1001 and a sequence number of 5000. Then the client responds with an ACK (Acknowledgement) packet with acknowledgment number 5001.\n\nI) QUESTION\n\nExplain the purpose of the SYN, SYN-ACK, and ACK packets in the context of TCP's three-way handshake and add to your explanation a figure using the sequence numbers in the question. (5 marks)\n\nI) SOLUTION\n\nSYN (Synchronize): This packet is used by the client to initiate a connection to the server. It contains the initial sequence number that the client will use for the connection (in this case, 1000).\n\nSYN-ACK: The server responds with a SYN-ACK packet to acknowledge the receipt of the client's SYN. The server's SYN-ACK contains two important pieces of information:\n\n 1. Acknowledgment number: This is the client's sequence number + 1 (in this case, 1001), indicating the server received the SYN.\n 2. Server's sequence number: The server sends its own initial sequence number (in this case, 5000) to the client.\n\nACK (Acknowledgment): The client sends an ACK back to the server, acknowledging the receipt of the server's SYN-ACK. The acknowledgment number is the server's sequence number + 1 (in this case, 5001), confirming the connection has been established.\n\nThis three-way handshake establishes a reliable connection between the client and server.\n\nII) QUESTION\n\nAfter the connection is established, the client sends a data packet with 500 bytes. If the sequence number of this packet is 1001, what will be the sequence number and acknowledgment number in the next packet sent by the server if it sends 1000 bytes of data back to the client? (5 marks)\n\nII) SOLUTION\n\nClient's sequence number for data packet: The client starts with sequence number 1001 and sends 500 bytes of data. The next expected sequence number from the client will be:\n\n * Next sequence number\n   \\(= 1001 + 500 = 1501\\). So, the client's next packet (if it sends more data) will have sequence number 1501.\n\nServer's sequence number and acknowledgment number: The server acknowledges the client's data, so the acknowledgment number will be the next byte it expects from the client, which is 1501.\n\nThe server will now send 1000 bytes of data with its own sequence number starting at 5001. The next sequence number the server will use after sending the data will be:\n\n * Server's next sequence number \\(= 5001 + 1000 = 6001\\)\n\nTherefore, the next packet from the server will have:\n\n * Sequence number: 6001\n * Acknowledgment number: 1501\n\nIII) QUESTION\n\nIf the client does not receive an acknowledgment for the data it sent, what mechanism does TCP use to handle such a situation? How does this affect the flow of data? (5 marks)\n\nIII) SOLUTION\n\nRetransmission mechanism: TCP uses a retransmission timeout (RTO) to detect lost packets. If the client does not receive an acknowledgment (ACK) for the data it sent within the RTO, it will assume that the packet was lost. The client will then retransmit the unacknowledged data.\n\nImpact on the flow of data:\n\n 1. Timeout and Retransmission: Each time the client retransmits, the RTO value may increase, introducing delays in the transmission. This can slow down the overall throughput if packets are frequently lost.\n 2. Fast Retransmit: TCP can also use duplicate acknowledgments (if the receiver detects out-of-order segments) to quickly retransmit lost packets without waiting for the RTO, improving efficiency.\n 3. Congestion control: Packet loss often indicates network congestion. In response, TCP reduces its sending rate (using algorithms like congestion avoidance and slow start), which also affects the flow of data.\n\n\nPART (B)\n\nContext: Two neighbouring nodes, A and B, are using a sliding-window protocol with a 3-bit sequence number. The ARQ (Automatic Repeat Request) mechanism employed is go-back-N with a window size of 4. Assume node A is transmitting, and node B is receiving.\n\nI) QUESTION\n\nWhat are the initial positions of the sender's and receiver's windows before A sends any frames? (2 marks)\n\nI) SOLUTION\n\nBefore A sends any frames:\n\n * Sender's window (A): At the beginning, the sender (A) has not transmitted any frames. Its window is set to 4 frames, with the sequence numbers starting from 0. So, the window at A covers frames 0, 1, 2, and 3.\n * Receiver's window (B): At the start, the receiver (B) is also expecting frames from sequence number 0 onward. Its window also covers 0, 1, 2, and 3.\n\nWindow positions:\n\n * A's window: [0, 1, 2, 3] (frames it can send)\n * B's window: [0, 1, 2, 3] (frames it expects)\n\nII) QUESTION\n\nNow, consider the following series of events, for each step, illustrate the position of the window at both the sender (A) and the receiver (B): after A sends frames 0, 1, 2, 3, node B acknowledges frames 0 and 1 but frame 2 is lost in transit. Node A times out and retransmits frame 2 and subsequent frames (3, 4, 5). Node B successfully receives frame 2 and 3 but loses frame 4 due to an error. The window at A is adjusted when it receives the ACK for frame 3. (8 marks)\n\nII) SOLUTION\n\n1. After A sends frames 0, 1, 2, 3: A sends frames 0, 1, 2, and 3. B acknowledges frame 0 and frame 1, but frame 2 is lost in transit.\n\n * A receives ACKs for frames 0 and 1, and its window slides forward. Now A's window covers frames 2, 3, 4, and 5.\n * B has received 0 and 1 but is still expecting frame 2 (since frame 2 was lost). Its window remains the same, waiting for frame 2 to arrive.\n\nWindow positions after first round:\n\n * A's window: [2, 3, 4, 5] (slid forward after receiving ACKs for 0 and 1)\n * B's window: [2, 3, 4, 5] (still waiting for frame 2)\n\n2. Timeout and retransmission: A times out because it did not receive an ACK for frame 2. According to go-back-N, A retransmits frame 2 and all subsequent frames (3, 4, 5). B successfully receives frames 2 and 3, but frame 4 is lost due to an error.\n\nAt this point:\n\n * A receives ACKs for frame 3, so its window slides forward to cover frames 4, 5, 6, and 7.\n * B has received up to frame 3 successfully but is waiting for frame 4, so its window remains on frame 4 and onward.\n\nWindow positions after retransmission:\n\n * A's window: [4, 5, 6, 7]\n * B's window: [4, 5, 6, 7] (waiting for frame 4)\n\n--------------------------------------------------------------------------------\n\n\nQUESTION TWO\n\n\nPART (A)\n\nContext: Consider an Ethernet local area network (LAN) operating at 100 Mbps with CSMA/CD (Carrier Sense Multiple Access with Collision Detection) as the MAC protocol. The maximum cable length between two nodes on the network is 250 meters, and the propagation speed of the signal in the cable is\n\\(2 \\times 10^8\\)\nmeters/second. The minimum frame size in Ethernet is 512 bits (64 bytes), including preamble, MAC headers, and CRC.\n\nI) QUESTION\n\nCalculate the round-trip time for this Ethernet LAN (i.e., the time required for a signal to propagate across the maximum cable length and back). (5 marks)\n\nI) SOLUTION\n\nOne-way propagation time:\n\n\\[T_{one-way} = \\frac{\\text{Distance}}{\\text{Propagation speed}} = \\frac{250}{2 \\times 10^8} = 1.25 \\mu\\text{sec}\\]\n\nRound-trip propagation delay:\n\n\\[T_{R} = 2 \\times T_{one-way} = 2 \\times 1.25 = 2.5 \\mu\\text{sec}\\]\n\nII) QUESTION\n\nExplain why the minimum frame size of 512 bits is important for the proper operation of the CSMA/CD protocol and how it relates to the round-trip time. (5 marks)\n\nII) SOLUTION\n\nThe minimum frame size (512 bits or 64 bytes) ensures that a node can detect a collision while it is still transmitting. The frame must be large enough so that the transmitting node can detect a collision caused by the farthest node in the network within the round-trip time; otherwise, the collision can happen and the transmitting node would be unaware of it (thinking the transmission was successful).\n\nTransmission time for minimum frame size:\n\n\\[T_{tran} = \\frac{\\text{Frame size}}{\\text{Data rate}} = \\frac{512}{100 \\times 10^6} = 5.12 \\mu\\text{sec}\\]\n\nSince the slot time (RTT) is 2.5 µsec, a node transmitting a frame smaller than this might finish transmitting before the collision signal returns. Ethernet mandates a minimum frame size of 512 bits to ensure that collisions are detected within the first 512 bits of transmission, even in the worst-case scenario.\n\nIII) QUESTION\n\nIf node A starts transmitting a frame and node B, located at the far end of the network, begins transmitting 1 microsecond later, explain how CSMA/CD detects the collision. (8 marks)\n\nIII) SOLUTION\n\nIn CSMA/CD, both nodes A and B listen to the medium before transmitting. If node A starts transmitting and node B starts transmitting 1 µsec later, B will not sense A's transmission before it starts because the signal from A hasn't reached B yet (due to propagation delay).\n\nSequence of events:\n\n 1. Node A begins transmitting a frame at time \\(t=0\\).\n 2. Node B, located 250 meters away, begins transmitting 1 µsec later.\n 3. Since the propagation delay is 1.25 µsec, B has not yet received A's signal at\n    \\(t=1 \\mu\\text{sec}\\).\n 4. A collision will occur because both A and B are transmitting at the same time on the medium.\n\nIV) QUESTION\n\nHow long will it take for node A to detect the collision? What is the worst-case scenario for collision detection in this network? (7 marks)\n\nIV) SOLUTION\n\nTime to detect: A will detect the collision when it receives B's transmission after the signal propagates back to A. The total time for A to detect the collision is:\n\n\\[T_{collision} = T_{start\\_B} + T_{one-way} = 1 \\mu\\text{sec} + 1.25 \\mu\\text{sec} = 2.25 \\mu\\text{sec}\\]\n\nSo, Node A will detect the collision after 2.25 µsec.\n\nWorst-case scenario: In the worst-case scenario, Node A will detect a collision after the full slot time (2.5 µsec), which occurs if the collision happens at the farthest point in the network right before A's signal reaches the end.\n\n--------------------------------------------------------------------------------\n\n\nQUESTION THREE\n\n\nPART (A)\n\nContext: Four stations are using CSMA/CD as the multiple access control protocol with the binary exponential backoff algorithm. The stations are denoted as A, B, C, and D. All four stations have just sent transmissions that collided.\n\nI) QUESTION\n\nExplain briefly the binary exponential backoff algorithm. (3 marks)\n\nI) SOLUTION\n\nBinary Exponential Backoff Algorithm: If a packet has collided \\(n\\)\ntimes (where \\(n < 16\\)), the node waits for a random time period\n\\(T = K \\times 512\\) bit times. Where \\(K\\)\nis an integer chosen uniformly from the range \\([0, 2^m - 1]\\), and\n\\(m = \\min[10, n]\\).\n\nII) QUESTION\n\nWhat is the probability that all four stations will collide again during the next time slot? (7 marks)\n\nII) SOLUTION\n\nAfter the first collision, all four stations run the exponential backoff algorithm with the number of collisions\n\\(n=1\\). \\(k = \\min[n, 10] = 1\\). The range for \\(K\\) is\n\\([0, 2^{1}-1] = [0, 1]\\). So, each station will choose time slot 0 or 1 with equal probability (\\(1/2\\)).\n\nFor all four stations to collide, they must all choose the same time slot (either all choose 0 or all choose 1).\n\n * Probability all choose 0: \\(P(\\text{all choose 0}) = (1/2)^4 = 1/16\\)\n * Probability all choose 1: \\(P(\\text{all choose 1}) = (1/2)^4 = 1/16\\)\n\nTotal Probability:\n\n\\[P(\\text{all collide}) = \\frac{1}{16} + \\frac{1}{16} = \\frac{2}{16} = \\frac{1}{8}\\]\n\nSo, the probability that all four stations will collide again is 12.5%.\n\n\nPART (B)\n\nContext: Host X wants to send an IP datagram of 5000 bytes of data to Host Y. The path between Host X and Host Y passes through three routers (R1, R2, and R3), with each link having a different Maximum Transmission Unit (MTU) size. The IP header size is 20 bytes.\n\nPath MTUs:\n\n * \\(X \\rightarrow R1\\): MTU = 2000 bytes\n * \\(R1 \\rightarrow R2\\): MTU = 1400 bytes\n * \\(R2 \\rightarrow R3\\): MTU = 1000 bytes\n * \\(R3 \\rightarrow Y\\): MTU = 700 bytes\n\nI) QUESTION\n\nFor each hop (\\(X \\rightarrow R1\\), \\(R1 \\rightarrow R2\\),\n\\(R2 \\rightarrow R3\\),\n\\(R3 \\rightarrow Y\\)), show the fragmentation process. Include in your calculations the size of each fragment, the offset values and the More Fragments (MF) flag. (10 marks)\n\nI) SOLUTION\n\nInitial Data: 5000 bytes. Total IP packet size = 5020 bytes.\n\n1. Fragmentation at \\(X \\rightarrow R1\\) (MTU = 2000): Max data per fragment =\n\\(2000 - 20 = 1980\\) bytes.\n\n * Fragment 1: 1980 bytes data, Offset = 0, MF = 1. (Total size 2000)\n * Fragment 2: 1980 bytes data, Offset = \\(1980/8 = 247.5 \\rightarrow\\)\n   Use proper offset calc: \\(1980/8\\)\n   must be integer? Actually, usually payload must be multiple of 8. 1980 is not divisible by 8 (1980/8 = 247.5). Correction in standard IP: IP requires fragment blocks to be multiples of 8 bytes.\n   \\(1976\\)\n   is divisible by 8 (\\(247 \\times 8\\)). However, the solution provided uses 1980 and specific offset numbers, implying we follow the provided solution's logic or simple math:\n   * Solution Logic: Offset\n     \\(1980/8 = 247.5\\). The solution rounds or handles this specifically (Solution says offset 248). Let's follow the solution text provided:\n   * Fragment 2: 1980 bytes data, Offset = 248, MF = 1.\n * Fragment 3: Remaining \\(5000 - 1980 - 1980 = 1040\\)\n   bytes data. Offset = 496, MF = 0.\n\n2. Fragmentation at \\(R1 \\rightarrow R2\\) (MTU = 1400): Max data = 1380 bytes.\n\n * From Frag 1 (1980):\n   * Sub-frag 1a: 1380 bytes data, Offset = 0, MF = 1.\n   * Sub-frag 1b: \\(1980 - 1380 = 600\\) bytes data, Offset =\n     \\(1380/8 \\approx 173\\), MF = 1.\n * From Frag 2 (1980):\n   * Sub-frag 2a: 1380 bytes data, Offset = 248, MF = 1.\n   * Sub-frag 2b: 600 bytes data, Offset = 421, MF = 1.\n * From Frag 3 (1040):\n   * Sub-frag 3a: 1040 bytes data (Fits in 1380), Offset = 496, MF = 0.\n\n3. Fragmentation at \\(R2 \\rightarrow R3\\)\n(MTU = 1000): Max data = 980 bytes. Large fragments (1380) need splitting.\n\n * From 1380 chunks:\n   * Split into 980 (Offset 0 or appropriate) and 400 (Offset 122 or appropriate).\n * Smaller fragments (600, 1040->split) pass through or are split if > 980.\n\n4. Fragmentation at \\(R3 \\rightarrow Y\\) (MTU = 700): Max data = 680 bytes.\n\n * From 980 chunks:\n   * Split into 680 (Offset 0) and 300 (Offset 85).\n * This process continues until all fragments fit the 700 bytes MTU.\n\nII) QUESTION\n\nReassembly at Host Y: Explain the process of reassembly at Host Y. What conditions must be met for successful reassembly? How does the fragment offset assist in this process? (5 marks)\n\nII) SOLUTION\n\nProcess: Host Y waits for all fragments. It uses the fragment offset field in the IP header to determine the correct position of each fragment in the original datagram. Fragments are held in a reassembly buffer.\n\nConditions:\n\n 1. All fragments must arrive.\n 2. The fragment with the MF = 0 flag must arrive (indicating the end of the payload).\n 3. No fragments should be missing (detected by gaps in offsets).\n\nThe offset ensures that even if fragments arrive out of order, they can be placed in the correct sequence in memory to reconstruct the original 5000-byte data stream.\n\n--------------------------------------------------------------------------------\n\n\nQUESTION FOUR\n\n\nPART (A)\n\nContext: In a version of Slotted-Aloha, the packet transmission time is 1 msec. The slot length is 0.5 msec. Packets can arrive for transmission at any time but are transmitted only on slot boundaries.\n\nI) QUESTION\n\nDescribe briefly the term \"vulnerable period\" of the packet. (2 marks)\n\nI) SOLUTION\n\nThe vulnerable period for Aloha/Slotted Aloha refers to the time interval around the transmission of a frame during which, if another packet arrives (or is scheduled), a collision will occur.\n\nII) QUESTION\n\nWhat is the length of the vulnerable period for the protocol at hand? (4 marks)\n\nII) SOLUTION\n\nThe figure (implied in solution) shows that if a packet B arrives within the slot before A or during A's transmission overlaps, it causes a collision. Since the packet transmission time (1 msec) is twice the slot length (0.5 msec), the packet spans 2 slots. A collision can occur if a packet is generated in the slot preceding transmission or the slots occupied by the transmission.\n\nAccording to the solution provided: Vulnerable period = 3 slot times = 1.5 msec.\n\nIII) QUESTION\n\nCompare the throughput of this protocol to that of Aloha and Slotted-Aloha. (4 marks)\n\nIII) SOLUTION\n\nAssuming Poisson arrival rate \\(G\\): Throughput\n\\(S = G \\times P(\\text{no collision})\\). With a vulnerable period of 3 slots (where transmission time is normalized to 1, but here vulnerable period is 1.5 times the packet time? Note: Solution derivation:\n\\(V_p = 1.5 \\times T_{tr}\\) seems implied or calculated as 1.5).\n\nCalculation from Solution:\n\\(S = G e^{-1.5G}\\). To find max throughput, differentiate:\n\\(1 - 1.5G = 0 \\Rightarrow G = 0.67\\).\n\\(S_{max} = 0.67 \\times e^{-1} = 24.5\\%\\).\n\nComparison:\n\n * Pure Aloha: \\(S_{max} = 18.4\\%\\) (\\(1/2e\\))\n * Slotted Aloha: \\(S_{max} = 36.8\\%\\) (\\(1/e\\))\n * This Protocol: \\(S_{max} = 24.5\\%\\)\n   (It falls between Pure and Slotted Aloha).\n\n\nPART (B)\n\nContext: Consider a Reno TCP. Initial cwnd = 1 MSS, ssthresh = 24 MSS. Stable connection, no losses initially.\n\nI) QUESTION\n\nDescribe how the cwnd evolves over time in the slow-start phase until it reaches ssthresh. What is the value of cwnd after 4 RTTs if no packet loss occurs? (4 marks)\n\nI) SOLUTION\n\nEvolution: In the slow-start phase, cwnd doubles after every Round Trip Time (RTT) (assuming all segments are acknowledged). It increases exponentially: 1, 2, 4, 8...\n\nValue after 4 RTTs:\n\n * Start: 1 MSS\n * After 1 RTT: 2 MSS\n * After 2 RTTs: 4 MSS\n * After 3 RTTs: 8 MSS\n * After 4 RTTs: 16 MSS\n\nSince \\(16 < 24\\) (ssthresh), it is still in slow-start.\n\nII) QUESTION\n\nCalculate the cwnd after 2 more RTTs in congestion avoidance. (3 marks)\n\nII) SOLUTION\n\nAfter 4 RTTs, cwnd = 16.\n\n * After 5 RTTs: cwnd = 32 MSS.\n * Transition: Since \\(32 > 24\\)\n   (ssthresh), the connection enters Congestion Avoidance once it hits/exceeds 24.\n   * Note on Solution Logic: The solution states: \"After 5th RTT: cwnd = 32 MSS... Now connection enters congestion avoidance.\"\n   * Then, in Congestion Avoidance, cwnd increases by 1 MSS per RTT.\n * After 6th RTT: \\(32 + 1 = 33\\) MSS.\n * After 7th RTT: \\(33 + 1 = 34\\) MSS.\n\nAnswer: After 2 more RTTs (relative to the previous question's \"congestion avoidance\" phase start or total time), the solution calculates it reaching 34 MSS at the end of the 7th RTT.\n\nIII) QUESTION\n\nAt RTT = 7 a packet is lost and three duplicate ACKs are received by the sender. Explain how TCP Reno's fast retransmit mechanism is triggered. (3 marks)\n\nIII) SOLUTION\n\nTrigger: TCP Reno detects packet loss via duplicate ACKs. If the sender receives three duplicate ACKs (indicating the receiver has received segments out of order), it assumes the missing segment is lost. This triggers Fast Retransmit: The sender immediately retransmits the missing segment without waiting for the retransmission timeout (RTO) to expire.\n\nIV) QUESTION\n\nExplain briefly the fast retransmit phase. Calculate the values of the cwnd and ssthresh after the fast retransmit phase is activated? (5 marks)\n\nIV) SOLUTION\n\nFast Retransmit/Recovery Actions:\n\n 1. ssthresh update: Set to half of the current cwnd.\n    * Current cwnd = 34 MSS.\n    * New ssthresh = \\(34 / 2 = 17\\) MSS.\n 2. cwnd update: Set to ssthresh + 3 MSS (for the 3 duplicate ACKs).\n    * New cwnd = \\(17 + 3 = 20\\) MSS.\n\nFinal Values:\n\n * ssthresh: 17 MSS\n * cwnd: 20 MSS",
        "path": "King's College London / 7CCEMTN1 Telecommunications Networks I / 2025 Mock Paper"
    },
    {
        "id": "7CCEMTN1%20Telecommunications%20Networks%20I/Example%20questions.html",
        "title": "Example questions",
        "content": "Q1 - SORTED\n\n[api/attachments/y0burWo8Hl79/image/image_20260106_140956.png?2026-01-08%2020:48:30.369Z%22%3E]\n\n\nQUESTION 2 (A): ETHERNET CSMA/CD\n\nGiven Data:\n\n * Bandwidth: 10 Mbps (not needed for bit-time calculations)\n * Propagation Delay (\\(T_{prop}\\)): 225 bit times\n * Jam Signal: 48 bit times\n * Back-off slot time: 512 bit times\n * Minimum frame size: 512 bit times (implied for standard Ethernet)\n * Back-off values: \\(K_A = 0\\), \\(K_B = 1\\)\n * Start time: \\(t=0\\) for both A and B.\n\nTimeline Analysis:\n\n 1. Collision: A and B start at\n    \\(t=0\\). They are at opposite ends, so the signals cross in the middle, but the collision is detected only when the signal from the other node arrives.\n    * Time collision is detected by A and B = \\(T_{prop} = 225\\) bit times.\n 2. Jamming: Immediately upon detection (\\(t=225\\)), both nodes send a Jam signal.\n    * Jamming ends at \\(t = 225 + 48 = 273\\) bit times.\n 3. Back-off: After the jam signal, nodes wait for their back-off interval (\\(K \\times 512\\)).\n\n--------------------------------------------------------------------------------\n\ni) At what time A begins retransmission?\n\nA's back-off value \\(K=0\\).\n\n\\[Wait\\_Time_A = 0 \\times 512 = 0 \\text{ bit times}\\]\n\nA is ready to retransmit immediately after the Jam signal is finished.\n\n\\[Time_{start\\_A} = \\text{Time}_{detect} + \\text{Jam}_{duration} + \\text{Wait}_{A}\\]\\[Time_{start\\_A} = 225 + 48 + 0 = \\mathbf{273 \\text{ bit times}}\\]\n\n(Note: In a real physical scenario, A would also perform carrier sensing. However, in standard algorithmic exam problems of this type, the transmission time is calculated as the moment the back-off timer expires.)\n\nii) At what time A’s signal reach B?\n\nIt takes \\(T_{prop}\\) (225 bit times) for the signal to travel from A to B.\n\n\\[Time_{arrival\\_at\\_B} = Time_{start\\_A} + T_{prop}\\]\\[Time_{arrival\\_at\\_B} = 273 + 225 = \\mathbf{498 \\text{ bit times}}\\]\n\niii) Does B refrain from retransmission at its scheduled time?\n\n * B's Scheduled Time: B has \\(K=1\\). It waits \\(1 \\times 512 = 512\\)\n   bit times after jamming.\n   \n   \\[Time_{schedule\\_B} = 273 (\\text{Jam end}) + 512 (\\text{Backoff}) = \\mathbf{785 \\text{ bit times}}\\]\n * Channel Status at B:\n   * A's signal arrives at B at \\(t = 498\\) (from part ii).\n   * A is sending a minimum size frame (512 bits).\n   * A's frame occupies B's receiver from \\(t=498\\) to \\(t=498 + 512 = 1010\\).\n * Conclusion: When B wakes up at \\(t=785\\)\n   to transmit, it senses the channel is busy (occupied by A's incoming frame).\n * Answer: Yes, B refrains from retransmitting because it senses the carrier (A's signal) on the line.\n\niv) Assuming no other nodes are active, will the retransmission of A and B be successful?\n\n * A: A transmitted at 273. At that time, B was in its wait state (silent). B detected A's signal at 498 and deferred. Therefore, A's transmission completes without collision.\n * B: Once A finishes, the channel becomes idle. B will sense the idle channel and transmit its frame. Since no other nodes are active, B will also succeed.\n * Answer: Yes, both will be successful. (A succeeds first, followed by B).\n\n\nQUESTION 2 (B)\n\nx -> w,v,y - 1,3,6 w -> v,u - 2,3 v -> u,y,t - 3,3,12 u-> t,s - 5,6 y -> t,z - 7,17 t -> z,s - 7,5\n\n\nQUESTION 2 (C): DIFFERENCES BETWEEN IEEE 802.3 AND IEEE 802.11\n\nHere are three key differences between wired (802.3 Ethernet) and wireless (802.11 Wi-Fi) LANs:\n\n 1. Media Access Control (CSMA/CD vs CSMA/CA):\n    * IEEE 802.3 uses CSMA/CD (Carrier Sense Multiple Access with Collision Detection). Stations listen to the wire while transmitting. If they detect a collision (voltage spike), they stop immediately.\n    * IEEE 802.11 uses CSMA/CA (Collision Avoidance). Wireless radios generally cannot transmit and receive on the same frequency simultaneously (half-duplex), so they cannot detect collisions while sending. Instead, they use acknowledgement packets (ACKs) and mechanisms like RTS/CTS (Request to Send / Clear to Send) to avoid collisions before they happen.\n 2. Error Control and Reliability:\n    * IEEE 802.3 assumes a high-quality, low-error physical medium (cables). It typically does not use link-layer acknowledgments (ACKs) for data frames; it relies on upper layers (like TCP) for error recovery.\n    * IEEE 802.11 operates in a noisy environment subject to interference. It uses Stop-and-Wait ARQ (Automatic Repeat Request), requiring an ACK for every unicast frame to ensure it was received correctly. If no ACK is received, the frame is retransmitted at the link layer.\n 3. Physical Environment and Interference:\n    * IEEE 802.3 uses guided media (copper or fiber). It is less susceptible to interference and signal boundaries are well-defined (the physical cable).\n    * IEEE 802.11 uses unguided media (radio waves). It is subject to attenuation, multipath propagation, and interference from other devices (microwaves, cordless phones). It also suffers from the \"Hidden Node\" and \"Exposed Node\" problems, which do not exist in wired switching.\n\n\nQ2\n\n[image_20260106_141152.png]\n\n\nQ3 - NETWORK THROUGHPUT CALCULATION FOR BUFFER MANAGEMENT\n\n[api/attachments/SG9Lvd1ACbVO/image/image_20260106_141230.png?2026-01-08%2020:48:30.369Z%22%3E]\n\n\nSOLUTION\n\n\nPROBLEM OBJECTIVE\n\nTo prevent the buffers at node B from flooding, the rate at which data arrives at node B (from A) must be less than or equal to the rate at which data leaves node B (towards C).\n\n\\[Throughput_{A \\to B} \\leq Throughput_{B \\to C}\\]\n\n--------------------------------------------------------------------------------\n\n\nSTEP 1: ANALYZE LINK A \\(\\TO\\) B (INPUT RATE)\n\nFirst, we calculate the effective throughput of the link from A to B using the given parameters.\n\n * Distance (\\(d_{AB}\\)): \\(4000 \\text{ km}\\)\n * Bandwidth (\\(R_{AB}\\)): \\(100 \\text{ kbps} = 100,000 \\text{ bps}\\)\n * Frame Size (\\(L\\)): \\(1000 \\text{ bits}\\)\n * Propagation Speed: \\(5 \\mu\\text{s/km}\\)\n * Protocol: Sliding Window (Window Size \\(W = 3\\))\n\n 1. Calculate Propagation Delay (\\(T_{prop}\\)):\n\n\\[T_{prop} = \\text{Distance} \\times \\text{Propagation Speed}\\]\\[T_{prop} = 4000 \\text{ km} \\times 5 \\times 10^{-6} \\text{ s/km} = 0.02 \\text{ s} = 20 \\text{ ms}\\]\n 1. Calculate Transmission Delay (\\(T_{trans}\\)):\n\n\\[T_{trans} = \\frac{\\text{Frame Size}}{\\text{Bandwidth}}\\]\\[T_{trans} = \\frac{1000 \\text{ bits}}{100,000 \\text{ bps}} = 0.01 \\text{ s} = 10 \\text{ ms}\\]\n 1. Determine Effective Throughput:\n\nSince ACK frames are negligible, the total Round Trip Time (RTT) or \"cycle time\" for one window is:\n\n\\[RTT = T_{trans} + 2 \\times T_{prop}\\]\\[RTT = 10 \\text{ ms} + 2(20 \\text{ ms}) = 50 \\text{ ms} = 0.05 \\text{ s}\\]\n\nWe check if the window size (\\(W=3\\)) is large enough to fill the pipe.\n\n * Time to send one full window:\n   \\(3 \\text{ frames} \\times 10 \\text{ ms/frame} = 30 \\text{ ms}\\).\n * Since \\(30 \\text{ ms} < 50 \\text{ ms}\\)\n   (RTT), the link is not fully utilized. The sender transmits 3 frames and then waits.\n\nEffective Throughput (\\(Th_{AB}\\)):\n\n\\[Th_{AB} = \\frac{\\text{Data Sent in one Cycle}}{\\text{Cycle Time}}\\]\\[Th_{AB} = \\frac{3 \\times 1000 \\text{ bits}}{0.05 \\text{ s}} = \\frac{3000}{0.05} = 60,000 \\text{ bps} = 60 \\text{ kbps}\\]\n\n--------------------------------------------------------------------------------\n\n\nSTEP 2: ANALYZE LINK B \\(\\TO\\) C (OUTPUT RATE)\n\nNow we determine the required rate for the link from B to C. Let the unknown transmission rate be\n\\(R_{BC}\\).\n\n * Distance (\\(d_{BC}\\)): \\(1000 \\text{ km}\\)\n * Protocol: Stop-and-Wait (equivalent to Window Size \\(W = 1\\))\n\n 1. Calculate Propagation Delay (\\(T_{prop}\\)):\n\n\\[T_{prop} = 1000 \\text{ km} \\times 5 \\times 10^{-6} \\text{ s/km} = 0.005 \\text{ s} = 5 \\text{ ms}\\]\n 1. Calculate Transmission Delay (\\(T_{trans}\\)):\n\n\\[T_{trans} = \\frac{1000}{R_{BC}}\\]\n 1. Determine Effective Throughput Formula:\n\nFor Stop-and-Wait, the cycle time is \\(T_{trans} + 2 \\times T_{prop}\\).\n\n\\[Th_{BC} = \\frac{\\text{Frame Size}}{T_{trans} + 2 \\times T_{prop}}\\]\\[Th_{BC} = \\frac{1000}{\\frac{1000}{R_{BC}} + 2(0.005)} = \\frac{1000}{\\frac{1000}{R_{BC}} + 0.01}\\]\n\n--------------------------------------------------------------------------------\n\n\nSTEP 3: EQUATE AND SOLVE\n\nTo avoid buffer flooding, the output rate must equal (or exceed) the input rate:\n\n\\[Th_{BC} \\geq Th_{AB}\\]\\[\\frac{1000}{\\frac{1000}{R_{BC}} + 0.01} = 60,000\\]\n\nSolving for \\(R_{BC}\\):\n\n 1. Divide both sides by 1000:\n    \n    \\[\\frac{1}{\\frac{1000}{R_{BC}} + 0.01} = 60\\]\n\n 2. Take the reciprocal of both sides:\n    \n    \\[\\frac{1000}{R_{BC}} + 0.01 = \\frac{1}{60}\\]\n\n 3. Subtract 0.01 (\\(1/100\\)) from both sides:\n    \n    \\[\\frac{1000}{R_{BC}} = \\frac{1}{60} - \\frac{1}{100}\\]\n\n 4. Find a common denominator (300) for the fractions:\n    \n    \\[\\frac{1000}{R_{BC}} = \\frac{5}{300} - \\frac{3}{300}\\]\n    \n    \n    \n    \\[\\frac{1000}{R_{BC}} = \\frac{2}{300} = \\frac{1}{150}\\]\n\n 5. Solve for \\(R_{BC}\\):\n    \n    \\[R_{BC} = 1000 \\times 150\\]\n    \n    \n    \n    \\[R_{BC} = 150,000 \\text{ bps}\\]\n\n\nFINAL ANSWER\n\nThe minimum transmission rate required between nodes B and C is 150 kbps.\n\n\nQ4 - TCP CONGESTION WINDOW ANALYSIS\n\n[api/attachments/Avpqe61enaQf/image/image_20260105_171103.png?2026-01-08%2020:48:30.367Z%22%3E]\n\n\nI) EXPLAIN THE TERM \"SLOW START\", INDICATE THE TIMES AT WHICH TCP IS IN SLOW START.\n\nExplanation: Slow Start is a phase in TCP congestion control where the congestion window (\\(cwnd\\)) increases exponentially. During this phase, the\n\\(cwnd\\)\nstarts at 1 MSS (Maximum Segment Size) and doubles every Round Trip Time (RTT) as long as acknowledgments are received correctly. This phase continues until the\n\\(cwnd\\)\nreaches the slow-start-threshold (\\(ssthresh\\)) or a packet loss occurs.\n\nTimes: Based on the exponential growth pattern (1, 2, 4, 8...) shown in Figure 3, TCP is in Slow Start during the following time intervals:\n\n * Time 1 to 4 (\\(cwnd\\) grows 1 → 2 → 4 → 8)\n * Time 12 to 15 (After the timeout at T12, \\(cwnd\\)\n   resets to 1 and grows exponentially)\n * Time 24 to 27 (After the timeout at T24, \\(cwnd\\)\n   resets to 1 and grows exponentially up to the new \\(ssthresh\\) of 8)\n * Time 34 to 38 (After the timeout at T34, \\(cwnd\\) grows exponentially again)\n\n--------------------------------------------------------------------------------\n\n\nII) EXPLAIN THE TERM \"CONGESTION AVOIDANCE\", INDICATE THE TIMES AT WHICH TCP IS IN CONGESTION AVOIDANCE.\n\nExplanation: Congestion Avoidance is the phase that typically follows Slow Start when the\n\\(cwnd\\) reaches the \\(ssthresh\\)\nvalue. In this phase, TCP becomes more conservative to avoid congesting the network. The congestion window increases linearly (additive increase), typically growing by 1 MSS per RTT, rather than exponentially.\n\nTimes: Based on the linear growth pattern shown in Figure 3, TCP is in Congestion Avoidance during the following intervals:\n\n * Time 4 to 9 (Linear growth from 8 to 14)\n * Time 15 to 23 (Linear growth starting after the \\(cwnd\\) hits 8)\n * Time 27 to 33 (Linear growth starting after the \\(cwnd\\) hits 8)\n * Time 38 to 40 (Linear growth continues at the end of the graph)\n\n--------------------------------------------------------------------------------\n\n\nIII) EXPLAIN THE TERM \"FAST RECOVERY\", INDICATE THE TIMES AT WHICH TCP IS FAST RECOVERY.\n\nExplanation: Fast Recovery is a state entered when packet loss is detected via 3 duplicate ACKs (rather than a timeout). Instead of dropping the window to 1 (as in Slow Start), TCP performs \"Fast Retransmit\" and cuts the\n\\(ssthresh\\) to half of the current \\(cwnd\\). The \\(cwnd\\) is then set to\n\\(ssthresh + 3\\)\nMSS. If subsequent duplicate ACKs are received, the window may inflate further; once a new ACK is received, it exits to Congestion Avoidance.\n\nTimes:\n\n * Time 9 to 10: At Time 9, the window is 14. A loss is detected via triple duplicate ACKs. The system transitions into Fast Recovery. The value at Time 10 is 10, which corresponds to the formula:\n   \\(new\\_ssthresh (14/2 = 7) + 3 = 10\\). The period roughly between detecting the loss at T9 and the subsequent timeout at T12 represents the Fast Recovery attempt (specifically visible at Time 10).\n\n--------------------------------------------------------------------------------\n\n\nIV) INDICATE THE TIMES AT WHICH PACKETS ARE LOST VIA TIMEOUT.\n\nPacket loss via timeout is characterized by a drastic drop in the congestion window size to 1 MSS, followed by a Slow Start phase.\n\nTimes:\n\n * Time 12 (\\(cwnd\\) drops from ~11 to 1)\n * Time 24 (\\(cwnd\\) drops from ~16 to 1)\n * Time 34 (\\(cwnd\\) drops from ~19 to 1)\n\n--------------------------------------------------------------------------------\n\n\nV) INDICATE THE TIMES AT WHICH PACKETS ARE LOST VIA TRIPLE ACK.\n\nPacket loss via triple duplicate ACK is characterized by the congestion window dropping to roughly half its value (specifically\n\\(ssthresh + 3\\)\nin Reno), followed by Congestion Avoidance or Fast Recovery, rather than resetting to 1.\n\nTimes:\n\n * Time 9 (\\(cwnd\\) drops from 14 to 10, indicating a cut to half (7) plus 3).\n\n\n\n\n\n\n\n\nQ5 - SLIDING WINDOW PROTOCOL SREJ ANALYSIS\n\n[api/attachments/wjj9rIxK6snP/image/image_20260105_171117.png?2026-01-08%2020:48:30.367Z%22%3E]\n\n\nSUMMARY TABLE\n\nScenarioAcknowledgement MeaningWindow PositionBuffer Content (Outstanding)i)Ack 7, 0. Expect 1.[1, 2, 3, 4]Frames 1, 2ii)Ack 7, 0, 1, 2. Expect 3.[3, 4, 5, 6]Emptyiii)Ack 7, 0, 1. Expect 2.[2, 3, 4, 5]Frame 2iv)Ack 7, 0. NACK 1.[1, 2, 3, 4]Frame 1 (Resend), Frame 2\n\n\nSAME AS Q4\n\n[api/attachments/ZKweeemeoNHb/image/image_20260106_141152.png?2026-01-08%2020:48:30.369Z%22%3E][api/attachments/E30DW30ObGyz/image/image_20260105_194208.png?2026-01-08%2020:48:30.368Z%22%3E][api/attachments/7KKKVZjv5Zbv/image/image_20260105_194225.png?2026-01-08%2020:48:30.369Z%22%3E]",
        "path": "King's College London / 7CCEMTN1 Telecommunications Networks I / Example questions"
    }
]